{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e677f80",
      "metadata": {
        "id": "3e677f80"
      },
      "outputs": [],
      "source": [
        "#  Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, cohen_kappa_score,\n",
        "                             matthews_corrcoef, classification_report)\n",
        "from transformers import PerceiverModel, PerceiverConfig\n",
        "import json\n",
        "import time\n",
        "import pprint\n",
        "from transformers import ViTForImageClassification, ViTConfig\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bhbrnkQ_tTuy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhbrnkQ_tTuy",
        "outputId": "0a8a0da9-0782-4bf6-8c08-a1866ddaeac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "DATA_DIR = '/content/gdrive/MyDrive/AI_Project_daraset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "udl_4BsPt3_4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udl_4BsPt3_4",
        "outputId": "efc05183-a738-4701-8b81-5fe212a22674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 109 zip files.\n",
            "1 — Unzipping 1.zip...\n",
            "2 — Unzipping 10.zip...\n",
            "3 — Unzipping 100.zip...\n",
            "4 — Unzipping 101.zip...\n",
            "5 — Unzipping 102.zip...\n",
            "6 — Unzipping 103.zip...\n",
            "7 — Unzipping 104.zip...\n",
            "Skipping 105.zip — not a valid zip file.\n",
            "Skipping 106.zip — not a valid zip file.\n",
            "Skipping 107.zip — not a valid zip file.\n",
            "Skipping 108.zip — not a valid zip file.\n",
            "Skipping 109.zip — not a valid zip file.\n",
            "13 — Unzipping 11.zip...\n",
            "14 — Unzipping 12.zip...\n",
            "15 — Unzipping 13.zip...\n",
            "16 — Unzipping 14.zip...\n",
            "17 — Unzipping 15.zip...\n",
            "18 — Unzipping 16.zip...\n",
            "19 — Unzipping 17.zip...\n",
            "20 — Unzipping 18.zip...\n",
            "21 — Unzipping 19.zip...\n",
            "22 — Unzipping 2.zip...\n",
            "23 — Unzipping 20.zip...\n",
            "24 — Unzipping 21.zip...\n",
            "25 — Unzipping 22.zip...\n",
            "26 — Unzipping 23.zip...\n",
            "27 — Unzipping 24.zip...\n",
            "28 — Unzipping 25.zip...\n",
            "29 — Unzipping 26.zip...\n",
            "30 — Unzipping 27.zip...\n",
            "31 — Unzipping 28.zip...\n",
            "32 — Unzipping 29.zip...\n",
            "33 — Unzipping 3.zip...\n",
            "34 — Unzipping 30.zip...\n",
            "35 — Unzipping 31.zip...\n",
            "36 — Unzipping 32.zip...\n",
            "37 — Unzipping 33.zip...\n",
            "38 — Unzipping 34.zip...\n",
            "39 — Unzipping 35.zip...\n",
            "40 — Unzipping 36.zip...\n",
            "41 — Unzipping 37.zip...\n",
            "42 — Unzipping 38.zip...\n",
            "43 — Unzipping 39.zip...\n",
            "44 — Unzipping 4.zip...\n",
            "45 — Unzipping 40.zip...\n",
            "46 — Unzipping 41.zip...\n",
            "47 — Unzipping 42.zip...\n",
            "48 — Unzipping 43.zip...\n",
            "49 — Unzipping 44.zip...\n",
            "50 — Unzipping 45.zip...\n",
            "51 — Unzipping 46.zip...\n",
            "52 — Unzipping 47.zip...\n",
            "53 — Unzipping 48.zip...\n",
            "54 — Unzipping 49.zip...\n",
            "55 — Unzipping 5.zip...\n",
            "56 — Unzipping 50.zip...\n",
            "57 — Unzipping 51.zip...\n",
            "58 — Unzipping 52.zip...\n",
            "59 — Unzipping 53.zip...\n",
            "60 — Unzipping 54.zip...\n",
            "61 — Unzipping 55.zip...\n",
            "62 — Unzipping 56.zip...\n",
            "63 — Unzipping 57.zip...\n",
            "64 — Unzipping 58.zip...\n",
            "65 — Unzipping 59.zip...\n",
            "66 — Unzipping 6.zip...\n",
            "67 — Unzipping 60.zip...\n",
            "68 — Unzipping 61.zip...\n",
            "69 — Unzipping 62.zip...\n",
            "70 — Unzipping 63.zip...\n",
            "71 — Unzipping 64.zip...\n",
            "72 — Unzipping 65.zip...\n",
            "73 — Unzipping 66.zip...\n",
            "74 — Unzipping 67.zip...\n",
            "75 — Unzipping 68.zip...\n",
            "76 — Unzipping 69.zip...\n",
            "77 — Unzipping 7.zip...\n",
            "78 — Unzipping 70.zip...\n",
            "79 — Unzipping 71.zip...\n",
            "80 — Unzipping 72.zip...\n",
            "81 — Unzipping 73.zip...\n",
            "82 — Unzipping 74.zip...\n",
            "83 — Unzipping 75.zip...\n",
            "84 — Unzipping 76.zip...\n",
            "85 — Unzipping 77.zip...\n",
            "86 — Unzipping 78.zip...\n",
            "87 — Unzipping 79.zip...\n",
            "88 — Unzipping 8.zip...\n",
            "89 — Unzipping 80.zip...\n",
            "90 — Unzipping 81.zip...\n",
            "91 — Unzipping 82.zip...\n",
            "92 — Unzipping 83.zip...\n",
            "93 — Unzipping 84.zip...\n",
            "94 — Unzipping 85.zip...\n",
            "95 — Unzipping 86.zip...\n",
            "96 — Unzipping 87.zip...\n",
            "97 — Unzipping 88.zip...\n",
            "98 — Unzipping 89.zip...\n",
            "99 — Unzipping 9.zip...\n",
            "100 — Unzipping 90.zip...\n",
            "101 — Unzipping 91.zip...\n",
            "102 — Unzipping 92.zip...\n",
            "103 — Unzipping 93.zip...\n",
            "104 — Unzipping 94.zip...\n",
            "105 — Unzipping 95.zip...\n",
            "106 — Unzipping 96.zip...\n",
            "107 — Unzipping 97.zip...\n",
            "108 — Unzipping 98.zip...\n",
            "109 — Unzipping 99.zip...\n",
            " Total valid subjects: 104 — IDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "output_dir = '/content/eeg_unzipped'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "zip_files = sorted([f for f in os.listdir(DATA_DIR) if f.endswith('.zip')])\n",
        "print(f\"Found {len(zip_files)} zip files.\")\n",
        "\n",
        "valid_subjects = []\n",
        "\n",
        "for i, zip_file in enumerate(zip_files, 1):\n",
        "    subject_id = int(os.path.splitext(zip_file)[0])\n",
        "    subject_dir = os.path.join(output_dir, str(subject_id))\n",
        "    os.makedirs(subject_dir, exist_ok=True)\n",
        "    try:\n",
        "        with zipfile.ZipFile(os.path.join(DATA_DIR, zip_file), 'r') as zip_ref:\n",
        "            print(f\"{i} — Unzipping {zip_file}...\")\n",
        "            zip_ref.extractall(subject_dir)\n",
        "            valid_subjects.append(subject_id)\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Skipping {zip_file} — not a valid zip file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error unzipping {zip_file}: {e}\")\n",
        "\n",
        "valid_subjects = sorted(valid_subjects)\n",
        "print(f\" Total valid subjects: {len(valid_subjects)} — IDs: {valid_subjects}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jltDVivYtT6P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jltDVivYtT6P",
        "outputId": "0432ca79-57fc-480c-c2fc-5d21d8d2b358"
      },
      "outputs": [],
      "source": [
        "# Path to the unzipped folder in Google Drive\n",
        "base_dir = '/content/eeg_unzipped'\n",
        "\n",
        "# Lists to hold data and subject IDs\n",
        "valid_subjects = []\n",
        "valid_subjects_data = []\n",
        "\n",
        "import psutil\n",
        "\n",
        "# Get virtual memory info\n",
        "vm = psutil.virtual_memory()\n",
        "\n",
        "# Print summary\n",
        "print(f\"Total RAM: {vm.total / 1e9:.2f} GB\")\n",
        "print(f\"Available RAM: {vm.available / 1e9:.2f} GB\")\n",
        "print(f\"Used RAM: {(vm.total - vm.available) / 1e9:.2f} GB\")\n",
        "print(f\"Percentage Used: {vm.percent}%\")\n",
        "\n",
        "\n",
        "# Walk through each subject folder\n",
        "for subject_folder in sorted(os.listdir(base_dir)):\n",
        "    subject_path = os.path.join(base_dir, subject_folder)\n",
        "\n",
        "    if os.path.isdir(subject_path):\n",
        "        npy_files = [f for f in os.listdir(subject_path) if f.endswith('.npy')]\n",
        "\n",
        "        if npy_files:\n",
        "            for npy_file in npy_files:\n",
        "                npy_path = os.path.join(subject_path, npy_file)\n",
        "\n",
        "                try:\n",
        "                    data = np.load(npy_path)\n",
        "                    valid_subjects.append(subject_folder)\n",
        "                    valid_subjects_data.append(data)\n",
        "                    print(f\" Loaded {npy_file} from subject {subject_folder}, shape: {data.shape}\")\n",
        "                    print(f\"Total RAM: {vm.total / 1e9:.2f} GB\")\n",
        "                    print(f\"Available RAM: {vm.available / 1e9:.2f} GB\")\n",
        "                    print(f\"Used RAM: {(vm.total - vm.available) / 1e9:.2f} GB\")\n",
        "                    print(f\"Percentage Used: {vm.percent}%\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load {npy_file} from subject {subject_folder}: {e}\")\n",
        "        else:\n",
        "            print(f\"⚠️ No .npy files found in {subject_folder}\")\n",
        "\n",
        "print(f\"\\n Total valid subjects loaded: {len(valid_subjects)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tD57MD8Gc2_4",
      "metadata": {
        "id": "tD57MD8Gc2_4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0dd5c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef0dd5c3",
        "outputId": "938b329b-11f7-4cc9-8e47-4235e1d22176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are using cuda now.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if with_gpu else \"cpu\")\n",
        "print(f'We are using {device} now.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f37b0c",
      "metadata": {
        "id": "a1f37b0c"
      },
      "outputs": [],
      "source": [
        "# 📂 Parameters\n",
        "DATA_DIR = '/content/eeg_unzipped'\n",
        "\n",
        "TRAIN_IDS = list(range(1, 74))\n",
        "DEV_IDS = list(range(75, 89))\n",
        "TEST_IDS = list(range(90, 104))\n",
        "no_feature = 64\n",
        "segment_length = 64\n",
        "n_class = 4\n",
        "removed_labels = [0,1,4,5,6,7,8,9,10]\n",
        "\n",
        "\n",
        "# ⚙️ Segmentation function\n",
        "# Segmentation function\n",
        "# Segmentation function\n",
        "def extract(input, n_classes, n_fea, time_window, moving):\n",
        "    xx, yy = input[:, :n_fea], input[:, n_fea:n_fea + 1]\n",
        "    new_x, new_y = [], []\n",
        "    for i in range(int((xx.shape[0] / moving) - 1)):\n",
        "        ave_y = np.average(yy[int(i * moving):int(i * moving + time_window)])\n",
        "        new_x.append(xx[int(i * moving):int(i * moving + time_window), :])\n",
        "        window = yy[int(i * moving):int(i * moving + time_window)]\n",
        "        label = np.bincount(window.astype(int).flatten()).argmax()\n",
        "        new_y.append(label)\n",
        "\n",
        "    new_x = np.array(new_x).reshape([-1, n_fea * time_window])\n",
        "    new_y = np.array(new_y).reshape([-1, 1])\n",
        "\n",
        "    return np.vstack((np.hstack((new_x, new_y)), np.hstack((new_x[-1], new_y[-1]))))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0ec22a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e0ec22a",
        "outputId": "f0bcd34c-c278-4b80-c8c4-b3d3cbea57c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Loading data...\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (257600, 65)\n",
            "shape = (257600, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255840, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255840, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (256320, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (256480, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (256160, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255840, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (209984, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (209984, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255520, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (255680, 65)\n",
            "shape = (208448, 65)\n",
            "shape = (259520, 65)\n",
            "shape = (255840, 65)\n",
            "shape = (259520, 65)\n",
            "✅ Train: (138264, 4096), Dev: (26976, 4096), Test: (26532, 4096)\n"
          ]
        }
      ],
      "source": [
        "def load_subjects(ids):\n",
        "    X_list, y_list = [], []\n",
        "    for sid in ids:\n",
        "        path = os.path.join(DATA_DIR, f'{sid}', f'{sid}.npy')\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"⚠️ Skipping missing {sid}\")\n",
        "            continue\n",
        "        data = np.load(path)\n",
        "        print(f'shape = {data.shape}')\n",
        "\n",
        "        # Segment the data\n",
        "        seg_data = extract(data, n_class, no_feature, segment_length, segment_length / 2)\n",
        "        X_list.append(seg_data[:, :-1])\n",
        "        y_list.append(seg_data[:, -1])\n",
        "\n",
        "    # Combine all data\n",
        "    X = np.vstack(X_list)\n",
        "    y = np.hstack(y_list)\n",
        "\n",
        "    # Keep only desired labels intended to classify against\n",
        "    mask = (y == 2) | (y == 3) | (y == 6) | (y == 7)\n",
        "    X, y = X[mask], y[mask]\n",
        "\n",
        "    # Map to class indices: 2 → 0, 3 → 1, 6 → 2, 7 → 3\n",
        "    label_map = {2: 0, 3: 1, 6: 2, 7: 3}\n",
        "    y = np.vectorize(label_map.get)(y)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Loading data...\")\n",
        "X_train_raw, y_train = load_subjects(TRAIN_IDS)\n",
        "X_dev_raw, y_dev = load_subjects(DEV_IDS)\n",
        "X_test_raw, y_test = load_subjects(TEST_IDS)\n",
        "\n",
        "print(f\" Train: {X_train_raw.shape}, Dev: {X_dev_raw.shape}, Test: {X_test_raw.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5WVxxqNHc4-l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "collapsed": true,
        "id": "5WVxxqNHc4-l",
        "outputId": "8ef752bd-2e2c-4c9b-dcde-c789706ce6b0"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YcZra-lPe1VA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "YcZra-lPe1VA",
        "outputId": "0a441296-ee6c-4ebf-bc36-128f72428e1a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rDRIrZP6f-NE",
      "metadata": {
        "id": "rDRIrZP6f-NE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff871f4",
      "metadata": {
        "id": "0ff871f4"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "scaler = StandardScaler().fit(X_train_raw.reshape(-1, no_feature))\n",
        "X_train = scaler.transform(X_train_raw.reshape(-1, no_feature)).reshape(-1, segment_length, no_feature)\n",
        "X_dev = scaler.transform(X_dev_raw.reshape(-1, no_feature)).reshape(-1, segment_length, no_feature)\n",
        "X_test = scaler.transform(X_test_raw.reshape(-1, no_feature)).reshape(-1, segment_length, no_feature)\n",
        "\n",
        "# Convert to torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "X_dev_tensor = torch.tensor(X_dev, dtype=torch.float32).to(device)\n",
        "y_dev_tensor = torch.tensor(y_dev, dtype=torch.long).to(device)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d590fd3",
      "metadata": {
        "id": "9d590fd3"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, num_layers=2, dropout=0.3):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # if bidirectional set to true:\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "        #self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17d5b6ad",
      "metadata": {
        "id": "17d5b6ad"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "epochs = 30\n",
        "hidden_size = 128\n",
        "lr = 0.001\n",
        "\n",
        "experiment_list = [\n",
        "    {'trial': 1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50, 'hidden_size': 128, 'num_layers': 1},\n",
        "    {'trial': 2, 'learning_rate': 1e-3, 'batch_size': 32, 'epochs': 50, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3},\n",
        "    {'trial': 3, 'learning_rate': 5e-4, 'batch_size': 64, 'epochs': 50, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3},\n",
        "    {'trial': 4, 'learning_rate': 1e-4, 'batch_size': 32, 'epochs': 100, 'hidden_size': 256, 'num_layers': 3, 'dropout': 0.4},\n",
        "    {'trial': 5, 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 60, 'hidden_size': 256, 'num_layers': 3, 'dropout': 0.5},\n",
        "]\n",
        "\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "dev_ds = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
        "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_ds, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "#uncomment to test LSTM alone\n",
        "# model = LSTMClassifier(input_size=no_feature, hidden_size=hidden_size, num_classes=n_class).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26033874",
      "metadata": {
        "id": "26033874"
      },
      "outputs": [],
      "source": [
        "def evaluate(loader, model, device):\n",
        "    model.eval()\n",
        "    preds, labels, probs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            out = model(xb)\n",
        "            soft_out = F.softmax(out, dim=1).cpu().numpy()\n",
        "            preds.extend(np.argmax(soft_out, axis=1))\n",
        "            probs.extend(soft_out)\n",
        "            labels.extend(yb.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec_weighted = precision_score(labels, preds, average='weighted')\n",
        "    rec_weighted = recall_score(labels, preds, average='weighted')\n",
        "    f1_weighted = f1_score(labels, preds, average='weighted')\n",
        "    prec_macro = precision_score(labels, preds, average='macro')\n",
        "    rec_macro = recall_score(labels, preds, average='macro')\n",
        "    f1_macro = f1_score(labels, preds, average='macro')\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    kappa = cohen_kappa_score(labels, preds)\n",
        "    conf_matrix = confusion_matrix(labels, preds).tolist()\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    y_true_onehot = encoder.fit_transform(np.array(labels).reshape(-1,1))\n",
        "    auc = roc_auc_score(y_true_onehot, np.array(probs))\n",
        "    report = classification_report(labels, preds, digits=4)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc, 'precision_weighted': prec_weighted, 'recall_weighted': rec_weighted, 'f1_weighted': f1_weighted,\n",
        "        'precision_macro': prec_macro, 'recall_macro': rec_macro, 'f1_macro': f1_macro,\n",
        "        'matthews_corrcoef': mcc, 'cohen_kappa': kappa, 'auc': auc,\n",
        "        'confusion_matrix': conf_matrix, 'classification_report': report\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9qaze9IitAhP",
      "metadata": {
        "id": "9qaze9IitAhP"
      },
      "outputs": [],
      "source": [
        "    # Evaluate with time tracking\n",
        "    def timed_evaluate(loader, model, device):\n",
        "        start_eval = time.perf_counter()\n",
        "        metrics = evaluate(loader, model, device)\n",
        "        end_eval = time.perf_counter()\n",
        "        metrics['time_seconds'] = end_eval - start_eval\n",
        "        print(f\"Evaluation time: {metrics['time_seconds']:.2f} seconds\")\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EbaAYLRDc8L0",
      "metadata": {
        "id": "EbaAYLRDc8L0"
      },
      "source": [
        "EEGNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fAQen5Uc9SN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fAQen5Uc9SN",
        "outputId": "172fb063-965d-4de8-9f59-f500d7c36f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: braindecode in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.10.0)\n",
            "Requirement already satisfied: mne_bids>=0.16 in /usr/local/lib/python3.11/dist-packages (from braindecode) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.3.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.14.0)\n",
            "Requirement already satisfied: skorch in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from braindecode) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.5.1)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.8.0)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.11/dist-packages (from braindecode) (4.3.0)\n",
            "Requirement already satisfied: linear_attention_transformer in /usr/local/lib/python3.11/dist-packages (from braindecode) (0.19.1)\n",
            "Requirement already satisfied: docstring_inheritance in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne>=1.10.0->braindecode) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne>=1.10.0->braindecode) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne>=1.10.0->braindecode) (0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne>=1.10.0->braindecode) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne>=1.10.0->braindecode) (1.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne>=1.10.0->braindecode) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (2.9.0.post0)\n",
            "Requirement already satisfied: axial-positional-embedding in /usr/local/lib/python3.11/dist-packages (from linear_attention_transformer->braindecode) (0.3.12)\n",
            "Requirement already satisfied: linformer>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from linear_attention_transformer->braindecode) (0.2.3)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.11/dist-packages (from linear_attention_transformer->braindecode) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from linear_attention_transformer->braindecode) (0.2.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->braindecode) (1.3.0)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb->braindecode) (3.11.15)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb->braindecode) (2.32.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb->braindecode) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.20.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne>=1.10.0->braindecode) (4.3.8)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.11/dist-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode) (0.11.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb->braindecode) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb->braindecode) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb->braindecode) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb->braindecode) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->skorch->braindecode) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb->braindecode) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne>=1.10.0->braindecode) (3.0.2)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from local-attention->linear_attention_transformer->braindecode) (0.2.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode) (2.22)\n"
          ]
        }
      ],
      "source": [
        "pip install braindecode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tUjlJt1pgoo9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tUjlJt1pgoo9",
        "outputId": "0819977b-e8f1-4d6b-bbe6-c4f43e745a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[-2.97979996e-01  1.10431059e-01  1.44580998e-01 ...  2.30435790e-01\n",
            "   -1.68691313e-03 -1.40127650e-01]\n",
            "  [-1.67037696e-01  6.38607348e-03  2.33512302e-01 ...  1.18483598e-01\n",
            "   -1.96797623e-01 -2.98740245e-01]\n",
            "  [-2.15462520e-02  1.10431059e-01  1.89046650e-01 ...  1.82456279e-01\n",
            "   -2.61834526e-01 -3.18566819e-01]\n",
            "  ...\n",
            "  [-8.94494917e-01 -1.04892735e+00 -9.07772762e-01 ... -9.69051981e-01\n",
            "   -1.31868421e+00 -9.53017200e-01]\n",
            "  [-1.11273208e+00 -1.03406378e+00 -8.18841458e-01 ... -1.03302466e+00\n",
            "   -1.17235117e+00 -9.72843774e-01]\n",
            "  [-7.78101761e-01 -5.88156699e-01 -3.89006824e-01 ... -5.53229553e-01\n",
            "   -5.87019043e-01 -3.58219968e-01]]\n",
            "\n",
            " [[-2.83430851e-01 -8.27953423e-02 -1.84597252e-02 ...  1.98449450e-01\n",
            "   -1.48019946e-01 -1.20301075e-01]\n",
            "  [-3.41627429e-01 -1.27386050e-01 -1.37034797e-01 ...  8.64972577e-02\n",
            "   -2.78093752e-01  1.84849455e-02]\n",
            "  [-4.14373151e-01 -3.94930298e-01 -5.07581895e-01 ... -2.65352488e-01\n",
            "   -4.73204462e-01 -4.09947777e-02]\n",
            "  ...\n",
            "  [ 5.31321236e-01  7.79291677e-01  9.74606499e-01 ...  2.46947963e+00\n",
            "    2.48597464e+00  3.58726834e+00]\n",
            "  [ 3.13084069e-01  7.34700969e-01  9.89428383e-01 ...  1.81375965e+00\n",
            "    1.93316096e+00  2.61576619e+00]\n",
            "  [-3.70725718e-01  1.10431059e-01  3.66909257e-01 ...  7.90196750e-01\n",
            "    1.02264431e+00  1.28738571e+00]]\n",
            "\n",
            " [[ 1.09396048e-01  1.84748905e-01  3.96553025e-01 ... -1.53400296e-01\n",
            "   -1.15501494e-01  1.77097541e-01]\n",
            "  [-6.61708606e-01 -6.32747406e-01 -4.03828708e-01 ... -6.65181745e-01\n",
            "   -6.03278269e-01 -5.16832563e-01]\n",
            "  [-8.21749195e-01 -6.47610976e-01 -4.92760011e-01 ... -8.57099789e-01\n",
            "   -8.63425882e-01 -9.53017200e-01]\n",
            "  ...\n",
            "  [ 1.09396048e-01  3.92838875e-01  1.59402882e-01 ...  1.04608747e+00\n",
            "    9.41348185e-01  1.92183609e+00]\n",
            "  [-1.67037696e-01  9.55674893e-02 -3.63784126e-03 ...  1.11006016e+00\n",
            "    1.05516277e+00  2.06062211e+00]\n",
            "  [ 2.25789203e-01  5.11747430e-01  4.26196793e-01 ...  1.04608747e+00\n",
            "    1.02264431e+00  1.76322349e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.06445408e-02 -2.33410651e-02  7.04715785e-02 ... -7.34344451e-02\n",
            "   -1.15501494e-01 -2.19433947e-01]\n",
            "  [-6.99710760e-03 -8.47749582e-03  5.56496945e-02 ... -2.81345659e-01\n",
            "   -3.26871430e-01 -6.15965435e-01]\n",
            "  [-5.06445408e-02 -8.27953423e-02 -1.84597252e-02 ... -2.33366148e-01\n",
            "   -2.61834526e-01 -3.38393394e-01]\n",
            "  ...\n",
            "  [ 3.71280647e-01  3.63111737e-01  3.22443606e-01 ... -2.01379807e-01\n",
            "   -4.08167559e-01 -5.36659138e-01]\n",
            "  [ 3.13084069e-01  3.18521029e-01  2.77977954e-01 ... -5.74412748e-02\n",
            "   -2.78093752e-01 -1.59954224e-01]\n",
            "  [ 5.16772091e-01  5.11747430e-01  4.26196793e-01 ...  8.64972577e-02\n",
            "   -1.31760720e-01  1.17617817e-01]]\n",
            "\n",
            " [[-6.99710760e-03 -6.79317730e-02 -9.25691449e-02 ... -1.21413956e-01\n",
            "   -2.29316075e-01 -1.00474501e-01]\n",
            "  [-6.99710760e-03 -8.27953423e-02 -9.25691449e-02 ... -1.85386637e-01\n",
            "   -4.89463688e-01 -1.99607373e-01]\n",
            "  [ 5.11994700e-02 -8.27953423e-02 -1.07391029e-01 ... -2.97338829e-01\n",
            "   -6.68315172e-01 -2.78913670e-01]\n",
            "  ...\n",
            "  [-1.08841118e-01 -1.42249620e-01 -1.81500449e-01 ... -3.77304680e-01\n",
            "   -6.68315172e-01 -6.35792010e-01]\n",
            "  [-2.15462520e-02 -6.79317730e-02 -9.25691449e-02 ... -3.61311510e-01\n",
            "   -6.03278269e-01 -4.97005989e-01]\n",
            "  [ 3.66503256e-02  2.12496428e-02 -3.32816092e-02 ... -2.97338829e-01\n",
            "   -4.89463688e-01 -4.97005989e-01]]\n",
            "\n",
            " [[ 5.45870380e-01  5.26610999e-01  4.41018677e-01 ...  1.18483598e-01\n",
            "   -8.29830423e-02  5.81380943e-02]\n",
            "  [ 5.45870380e-01  5.26610999e-01  4.26196793e-01 ...  1.18483598e-01\n",
            "   -8.29830423e-02  5.81380943e-02]\n",
            "  [ 5.45870380e-01  5.11747430e-01  4.11374909e-01 ...  5.45109171e-02\n",
            "   -9.92422681e-02 -6.08213521e-02]\n",
            "  ...\n",
            "  [-2.54332563e-01 -2.76021743e-01 -1.96322333e-01 ... -5.74412748e-02\n",
            "    1.45723127e-02 -1.00474501e-01]\n",
            "  [-1.96135985e-01 -2.16567466e-01 -1.07391029e-01 ...  7.05040874e-02\n",
            "    1.44646119e-01  5.81380943e-02]\n",
            "  [-1.52488552e-01 -2.01703897e-01 -6.29253770e-02 ...  1.02490428e-01\n",
            "    1.44646119e-01  5.81380943e-02]]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  Reshape to match EEGNet input\n",
        "scaler = StandardScaler().fit(X_train_raw.reshape(-1, no_feature))\n",
        "X_train = scaler.transform(X_train_raw.reshape(-1, no_feature)).reshape(-1, segment_length, no_feature)\n",
        "X_dev = scaler.transform(X_dev_raw.reshape(-1, no_feature)).reshape(-1, segment_length, no_feature)\n",
        "X_test = scaler.transform(X_test_raw.reshape(-1, no_feature)).reshape(-1, segment_length, no_feature)\n",
        "print(X_train)\n",
        "X_train = X_train.transpose(0, 2, 1).astype(np.float32)\n",
        "X_dev = X_dev.transpose(0, 2, 1).astype(np.float32)\n",
        "X_test = X_test.transpose(0, 2, 1).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ZQct_bddQqE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZQct_bddQqE",
        "outputId": "ba796f8c-5951-4e11-82a2-012fa6769ac2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/braindecode/models/eegnet.py:129: RuntimeWarning: The parameter `third_kernel_size` is deprecated and will be removed in a future version.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 — Loss: 191032.69 — Accuracy: 0.2757\n",
            "Epoch 2/30 — Loss: 188742.49 — Accuracy: 0.3004\n",
            "Epoch 3/30 — Loss: 186814.02 — Accuracy: 0.3229\n",
            "Epoch 4/30 — Loss: 185586.05 — Accuracy: 0.3350\n",
            "Epoch 5/30 — Loss: 184771.48 — Accuracy: 0.3417\n",
            "Epoch 6/30 — Loss: 184273.64 — Accuracy: 0.3474\n",
            "Epoch 7/30 — Loss: 183783.26 — Accuracy: 0.3518\n",
            "Epoch 8/30 — Loss: 183003.05 — Accuracy: 0.3598\n",
            "Epoch 9/30 — Loss: 182229.27 — Accuracy: 0.3656\n",
            "Epoch 10/30 — Loss: 181875.99 — Accuracy: 0.3683\n",
            "Epoch 11/30 — Loss: 181518.89 — Accuracy: 0.3705\n",
            "Epoch 12/30 — Loss: 181167.04 — Accuracy: 0.3711\n",
            "Epoch 13/30 — Loss: 181121.50 — Accuracy: 0.3725\n",
            "Epoch 14/30 — Loss: 180830.12 — Accuracy: 0.3756\n",
            "Epoch 15/30 — Loss: 180583.30 — Accuracy: 0.3760\n",
            "Epoch 16/30 — Loss: 180506.24 — Accuracy: 0.3777\n",
            "Epoch 17/30 — Loss: 180357.87 — Accuracy: 0.3771\n",
            "Epoch 18/30 — Loss: 180120.76 — Accuracy: 0.3789\n",
            "Epoch 19/30 — Loss: 179825.58 — Accuracy: 0.3817\n",
            "Epoch 20/30 — Loss: 179984.25 — Accuracy: 0.3813\n",
            "Epoch 21/30 — Loss: 179774.36 — Accuracy: 0.3805\n",
            "Epoch 22/30 — Loss: 179594.75 — Accuracy: 0.3833\n",
            "Epoch 23/30 — Loss: 179447.47 — Accuracy: 0.3833\n",
            "Epoch 24/30 — Loss: 179308.14 — Accuracy: 0.3845\n",
            "Epoch 25/30 — Loss: 179170.29 — Accuracy: 0.3854\n",
            "Epoch 26/30 — Loss: 179156.99 — Accuracy: 0.3854\n",
            "Epoch 27/30 — Loss: 178830.53 — Accuracy: 0.3885\n",
            "Epoch 28/30 — Loss: 178997.02 — Accuracy: 0.3888\n",
            "Epoch 29/30 — Loss: 178763.91 — Accuracy: 0.3881\n",
            "Epoch 30/30 — Loss: 178693.87 — Accuracy: 0.3899\n",
            "\n",
            "📊 Train metrics\n",
            "Accuracy: 0.40576722791182085\n",
            "Precision: 0.41678380126732517\n",
            "Recall: 0.40576722791182085\n",
            "F1: 0.39765720581964614\n",
            "Confusion matrix:\n",
            " [[17798  2976  8643  5423]\n",
            " [10360  7477 10117  6338]\n",
            " [ 9162  3067 15142  6883]\n",
            " [ 7185  2779  9228 15686]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.51      0.45     34840\n",
            "           1       0.46      0.22      0.30     34292\n",
            "           2       0.35      0.44      0.39     34254\n",
            "           3       0.46      0.45      0.45     34878\n",
            "\n",
            "    accuracy                           0.41    138264\n",
            "   macro avg       0.42      0.41      0.40    138264\n",
            "weighted avg       0.42      0.41      0.40    138264\n",
            "\n",
            "\n",
            "📊 Dev metrics\n",
            "Accuracy: 0.33518683274021355\n",
            "Precision: 0.33780213198177517\n",
            "Recall: 0.33518683274021355\n",
            "F1: 0.32408980765951306\n",
            "Confusion matrix:\n",
            " [[2880  717 1594 1442]\n",
            " [2200 1076 1666 1913]\n",
            " [1942  681 2172 1907]\n",
            " [1566  540 1766 2914]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.43      0.38      6633\n",
            "           1       0.36      0.16      0.22      6855\n",
            "           2       0.30      0.32      0.31      6702\n",
            "           3       0.36      0.43      0.39      6786\n",
            "\n",
            "    accuracy                           0.34     26976\n",
            "   macro avg       0.34      0.34      0.32     26976\n",
            "weighted avg       0.34      0.34      0.32     26976\n",
            "\n",
            "\n",
            "📊 Test metrics\n",
            "Accuracy: 0.3345771144278607\n",
            "Precision: 0.34712012483505256\n",
            "Recall: 0.3345771144278607\n",
            "F1: 0.32563517209656867\n",
            "Confusion matrix:\n",
            " [[2428  538 1740 1859]\n",
            " [1864 1125 1868 1844]\n",
            " [1639  551 2532 1976]\n",
            " [1325  534 1917 2792]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.37      0.35      6565\n",
            "           1       0.41      0.17      0.24      6701\n",
            "           2       0.31      0.38      0.34      6698\n",
            "           3       0.33      0.43      0.37      6568\n",
            "\n",
            "    accuracy                           0.33     26532\n",
            "   macro avg       0.35      0.34      0.33     26532\n",
            "weighted avg       0.35      0.33      0.33     26532\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from braindecode.models import EEGNetv4\n",
        "from braindecode.util import set_random_seeds\n",
        "\n",
        "# Load data and ensure correct shape\n",
        "# X shape must be (samples, channels, time)\n",
        "X_train = X_train.transpose(0, 2, 1).astype(np.float32)\n",
        "X_dev = X_dev.transpose(0, 2, 1).astype(np.float32)\n",
        "X_test = X_test.transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_dev = y_dev.astype(np.int64)\n",
        "y_test = y_test.astype(np.int64)\n",
        "\n",
        "#  Dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(TensorDataset(torch.tensor(X_dev), torch.tensor(y_dev)), batch_size=batch_size)\n",
        "test_loader = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)), batch_size=batch_size)\n",
        "\n",
        "#  EEGNet Model\n",
        "set_random_seeds(seed=2023, cuda=torch.cuda.is_available())\n",
        "\n",
        "n_channels = X_train.shape[1]\n",
        "input_time_length = X_train.shape[2]\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "\n",
        "\n",
        "n_channels = 64  # From your data\n",
        "sfreq = 160      # Sampling frequency (Hz)\n",
        "duration = X_train.shape[2] / sfreq  # Window duration in seconds\n",
        "\n",
        "\n",
        "model = EEGNetv4(\n",
        "    n_chans=64,\n",
        "    n_outputs=4,\n",
        "    input_window_samples=X_train.shape[2],  # Time steps\n",
        "    pool_mode='mean',\n",
        "    drop_prob=0.5,  # Increased dropout\n",
        "    final_conv_length='auto'\n",
        ").cuda()\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_normal_(m.weight, gain=0.1)\n",
        "model.apply(weights_init)\n",
        "# ⚙️ Training setup\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 🏋️ Training loop\n",
        "n_epochs = 30\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_loss, correct = 0, 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.cuda(), yb.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * xb.size(0)\n",
        "        correct += (out.argmax(dim=1) == yb).sum().item()\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs} — Loss: {train_loss:.2f} — Accuracy: {acc:.4f}\")\n",
        "\n",
        "def evaluate(loader, split=\"\"):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.cuda()\n",
        "            out = model(xb)\n",
        "            preds = out.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(yb.numpy())\n",
        "    print(f\"\\n📊 {split} metrics\")\n",
        "    print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
        "    print(\"Precision:\", precision_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
        "    print(\"Recall:\", recall_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
        "    print(\"F1:\", f1_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
        "    print(\"Classification report:\\n\", classification_report(all_labels, all_preds))\n",
        "\n",
        "evaluate(train_loader, \"Train\")\n",
        "evaluate(dev_loader, \"Dev\")\n",
        "evaluate(test_loader, \"Test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FS2TiXL9yqwA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "FS2TiXL9yqwA",
        "outputId": "d5fb39e6-58b2-4146-adc3-bc437fa388c9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABOcAAAGJCAYAAADBpFc8AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAexJJREFUeJzt3XlcVdX+//E3ogwOQA6AXBVxBmdxIs2cEhUtc0jLkhzTC5bSVbNr5lBZlmOiZg54S65TmaUmIqZWaipKOaS3jNJScApQS1DYvz/8sb8eAUUFNurr+Xicx0P2+py9197q+bA/Z6217QzDMAQAAAAAAACgwBWxugMAAAAAAADAg4riHAAAAAAAAGARinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hzuG88//7wqV65sdTfy1D//+U899thjVncjW3Z2dpowYYLV3bjnnDt3TiVKlNCGDRus7goAmKZOnapatWopIyPD6q5kUblyZT3//PNWd6PQ6NOnj5566imruwEAt417m3vHK6+8ombNmlndjQcKxTnkOzs7u1y9tm7danVX883hw4c1YcIE/frrr7l+T3x8vBYuXKhXX31VktS6detcXcfcJJXIyEjNnDnzzk4mD1y8eFGvv/66OnbsqNKlS8vOzk4RERHZxn744Yd69NFH5eHhIUdHR/n4+Kh///7ZXsvk5GSNHj1a1atXl7Ozs7y9vTVw4EAdP37cJu7TTz9V7969VaVKFRUvXlw1a9bUyy+/rKSkpGz7cOHCBY0ePVo+Pj5ydHTUP/7xD/Xs2VN//fVXjuc4ePBg2dnZqUuXLjbby5Qpo0GDBum111676TUCkPciIiJsPi+dnJzk5eWlwMBAzZ49WxcuXLC6i3ft5MmTmjBhguLi4nL9npSUFL3zzjsaM2aMihQpoueffz5X+SY3BbMNGzZYerNz5MgRjR49Wg0aNFCpUqVUvnx5BQUFae/evTm+Z8WKFQoICFCJEiXk5uamhx9+WFu2bLGJSUxMVP/+/eXu7i5nZ2c1atRIq1atyrKvypUr53j9qlevniU+MTFRL7zwgv7xj3/IyclJlStX1sCBA21ixowZo08++UTff//9HV4VAPmlIO97/vrrL02YMMGSeyjubWzt2bNHoaGhql27tkqUKKFKlSrpqaee0v/+979s4zMyMjRv3jw1aNBAzs7OKlOmjNq2bWvzuf7rr7/meE2WL19us7/c3i+dOHFCEydOVNOmTfXQQw+pbNmyat26tTZv3pyljyNGjND333+vzz///O4vEHKlqNUdwP3vo48+svn5P//5j6Kjo7Ns9/X1vavjfPjhh4XyG3/pWgKbOHGiWrdunevRfbNmzZKPj4/atGkjSfr3v/+tQYMGme179uzR7Nmz9eqrr9pcu3r16t1y35GRkTp48KBGjBhxW+eRV86ePatJkyapUqVKql+//k1/qdi/f798fHz0+OOP66GHHlJ8fLw+/PBDrVu3Tt9//728vLwkXUtyjz32mA4fPqx//vOfqlGjhn7++WfNnTtXUVFR+vHHH1WqVClJ0pAhQ+Tl5aVnn31WlSpV0oEDBzRnzhxt2LBB+/btk7Ozs3n85ORkPfroo/r99981ZMgQVatWTWfOnNHXX3+t1NRUFS9ePEuf9+7dq4iICDk5OWV7TkOHDtXs2bO1ZcsWtW3b9i6uJIA7MWnSJPn4+OjKlStKSEjQ1q1bNWLECE2fPl2ff/55rj5HC6uTJ09q4sSJqly5sho0aJCr9yxevFhXr17V008/LUl64YUX1L59e7M9Pj5e48eP15AhQ/TII4+Y26tWrXrLfW/YsEHh4eGWFegWLlyoRYsWqUePHvrnP/+p5ORkffDBB2revLk2btxoc56SNGHCBE2aNEk9e/bU888/rytXrujgwYP6448/zJiUlBS1bNlSiYmJeumll+Tp6amVK1fqqaee0rJly/TMM8+YsTNnztTFixdtjvHbb79p3Lhx6tChg832EydOqEWLFpKu5Yl//OMfOnnypHbv3m0T17BhQzVu3FjTpk3Tf/7znzy5TgDyRkHd90jXinMTJ06UdK3QVZC4t7H1zjvv6Ntvv1WvXr1Ur149JSQkaM6cOWrUqJF27dqlOnXq2MQPGDBAy5YtU79+/RQaGqpLly5p//79On36dJZ9P/300+rcubPNtoCAAJufc3u/tHbtWr3zzjvq1q2bgoODdfXqVf3nP//RY489psWLF6t///7mPj09PfXEE0/ovffe0+OPP55Xlwo3YwAFLCQkxMjNP71Lly4VQG8KxqpVqwxJxldffZWr+LS0NKNs2bLGuHHj8myf1wsKCjK8vb1v+33Xk2S8/vrrd/Tey5cvG6dOnTIMwzD27NljSDKWLFmS6/fv3bvXkGRMmTLF3Pbtt98akow5c+bYxC5evNiQZHz66afmtuyu2dKlSw1JxocffmizfdiwYYabm5vxyy+/5KpvGRkZRkBAgDFgwADD29vbCAoKyjauTp06xnPPPZerfQLIG0uWLDEkGXv27MnSFhMTYzg7Oxve3t7GX3/9ZUHv8sadfKbWq1fPePbZZ/N0n5lym/Nvxtvb2wgODr6j9+7du9e4cOGCzbazZ88a5cqVM1q0aGGzfefOnYadnZ0xffr0m+5z6tSphiQjJibG3Jaenm40adLE8PT0NFJTU2/6/smTJxuSjG+//dZme6dOnQwfHx/j7Nmztzyv9957zyhRokSWcwNQuOTFZ2BOzpw5c1e/j98N7m1sffvtt1k++//3v/8Zjo6ORt++fW22r1ixIsu9SXbi4+MNSca77757R33K7n7p4MGDxpkzZ2ziLl++bNSqVcuoUKFCln2sXr3asLOzM44dO3ZHfcDtYVorCoXWrVurTp06io2NVatWrVS8eHFzyPPatWsVFBQkLy8vOTo6qmrVqpo8ebLS09Nt9nHjmnOZQ4Hfe+89LViwQFWrVpWjo6OaNGmiPXv23LJPV65c0cSJE1W9enU5OTmpTJkyatmypaKjo23ijhw5op49e6p06dJycnJS48aNbYb/RkREqFevXpKkNm3a5Go4+zfffKOzZ89m+UY/N+bOnavatWvL0dFRXl5eCgkJsZmu2bp1a61fv16//fab2ZfM65aWlqbx48fL399frq6uKlGihB555BF99dVXuTr2kSNHskwhzY6jo6M8PT1v+9wyZfb3+vNKSUmRJHl4eNjEli9fXpJsRsNl9+3ik08+KUn68ccfzW1JSUlasmSJhgwZIh8fH6WlpSk1NfWmffvoo4908OBBvfnmmzeNe+yxx/TFF1/IMIybxgEoGG3bttVrr72m3377TR9//LFN260+5/fu3Ss7OzstXbo0y36joqJkZ2endevW3fT477//vmrXrq3ixYvroYceUuPGjRUZGWkT88cff2jAgAHmtJXatWtr8eLFZvvWrVvVpEkTSVL//v3Nz/iclg2Qro2K++GHH+4o36xatUr+/v5ydnZW2bJl9eyzz9qMMHv++ecVHh4uyXaqV6b33ntPDz/8sMqUKSNnZ2f5+/tr9erVuTr2sWPHdOzYsVvG+fv7q2TJkjbbypQpo0ceecTm8166NsrN09NTL730kgzDyDLiLdPXX3+tcuXK2Yx8LlKkiJ566iklJCRo27ZtN+1TZGSkfHx89PDDD5vbjhw5oi+//FKjRo1SmTJldPnyZV25ciXHfTz22GO6dOlSlt9JABR+GRkZmjlzpmrXri0nJyd5eHjohRde0J9//mkTt3fvXgUGBqps2bJydnaWj4+PBgwYIOnafU65cuUkSRMnTszVFFDubfLv3ubhhx+Wg4ODzbbq1aurdu3aWXLN9OnT1bRpUz355JPKyMjQpUuXbrn/S5cuKS0tLVd9zpTd/VLt2rVVtmxZmzhHR0d17txZv//+e5blPTL/vtauXXtbx8adoTiHQuPcuXPq1KmTGjRooJkzZ5pDniMiIlSyZEmFhYVp1qxZ8vf31/jx4/XKK6/kar+RkZF699139cILL+iNN97Qr7/+qu7du9/0l17p2tSWiRMnqk2bNpozZ47+/e9/q1KlStq3b58Zc+jQITVv3lw//vijXnnlFU2bNk0lSpRQt27dtGbNGklSq1at9OKLL0qSXn31VX300Uf66KOPbjqcfceOHbKzs1PDhg1zdY7X9zkkJEReXl6aNm2aevTooQ8++EAdOnQwz/ff//63GjRooLJly5p9yVyjISUlRQsXLlTr1q31zjvvaMKECTpz5owCAwNztX6Rr6+v+vXrd1t9zq1z587p9OnT2rt3rznkul27dmZ748aNVaJECb322mvasmWL/vjjD23btk2jR49WkyZNbvnLQEJCgiTZJKxvvvlGly9fVrVq1dSzZ08VL15czs7OatGiRbbX48KFCxozZoxeffXVWxYf/f39lZSUpEOHDuX2EgDIZ88995wkadOmTea23HzON27cWFWqVNHKlSuz7HPFihV66KGHFBgYmONxP/zwQ7344ovy8/PTzJkzNXHiRDVo0EDfffedGZOYmKjmzZtr8+bNCg0N1axZs1StWjUNHDjQ/Az39fXVpEmTJF2bvp/5Gd+qVascj71jxw5JUqNGjXJ5la6JiIjQU089JXt7e02ZMkWDBw/Wp59+qpYtW5o3Ai+88IK58HdmX66f2jVr1iw1bNhQkyZN0ltvvaWiRYuqV69eWr9+/S2P365dO5sccLsSEhKy3KDExMSoSZMmmj17tsqVK2euUTdnzhybuNTUVJsvfDJlLnMQGxub43H379+vH3/80WbqqyRzvR8PDw+1a9dOzs7OcnZ2VqdOnbJd08nPz0/Ozs769ttvc3W+AAqPF154QaNGjVKLFi00a9Ys9e/fX8uWLVNgYKD5+/rp06fVoUMH/frrr3rllVf0/vvvq2/fvtq1a5ckqVy5cpo3b56ka18wZ36+du/ePcfjcm9TsPc2hmEoMTHRJtekpKRo9+7datKkiV599VW5urqqZMmSOf4OIV0rvpYsWVJOTk5q0qSJze8oN7rV/VJOEhISVLx48SzL9bi6uqpq1arkmoJi8cg9PICyG9796KOPGpKM+fPnZ4nPbnrRCy+8YBQvXty4fPmyuS04ONhmOHPmUOAyZcoY58+fN7evXbvWkGR88cUXN+1n/fr1c5ySmKldu3ZG3bp1bfqRkZFhPPzww0b16tXNbbc7TPvZZ581ypQpc9OYG/d5+vRpw8HBwejQoYORnp5uxs2ZM8eQZCxevNjcltPQ76tXr2YZkv3nn38aHh4exoABA2y2K5uh35KMRx999NYneJ3cTpdydHQ0JJl/p7Nnz84Ss27dOqN8+fJmnCQjMDAwV9N+Bg4caNjb2xv/+9//zG3Tp083j9e0aVNj2bJlxty5cw0PDw/joYceMk6ePGmzj3/961+Gj4+P+e/hZtNad+zYYUgyVqxYccu+AcgbN5vWmsnV1dVo2LCh+XNuP+fHjh1rFCtWzCbfpKamGm5ublk+P2/0xBNPGLVr175pzMCBA43y5ctnmfLYp08fw9XV1cyVtzsFddy4cYakm35O3rjPtLQ0w93d3ahTp47x999/m3Hr1q0zJBnjx483t91sSteN+T0tLc2oU6eO0bZtW5vt2U1r9fb2vuMpTNu3bzfs7OyM1157zdx2/vx58/O+ZMmSxrvvvmusWLHC6NixY5bfT4YPH24UKVLE+PXXX23226dPH0OSERoamuOxX375ZUOScfjwYZvtL774onn8jh07GitWrDDeffddo2TJkkbVqlWzXeqjRo0aRqdOne7oGgAoGDd+Bn799deGJGPZsmU2cRs3brTZvmbNmlvmq9ud1sq9zTX5fW+T6aOPPjIkGYsWLTK37du3z/ys9/DwMObOnWssW7bMaNq0qWFnZ2d8+eWXZuxvv/1mdOjQwZg3b57x+eefGzNnzjQqVapkFClSxFi3bl22x8zN/dKNfvrpJ8PJySnH5XY6dOhg+Pr63ubZ404wcg6FhqOjo80ilJmu/3b6woULOnv2rB555BH99ddfOnLkyC3327t3bz300EPmz5mLWf/yyy83fZ+bm5sOHTqkn376Kdv28+fPa8uWLXrqqafMfp09e1bnzp1TYGCgfvrpJ5vpPbfj3LlzNn3Ojc2bNystLU0jRoxQkSL/91978ODBcnFxydVIBHt7e3NIdkZGhs6fP6+rV6+qcePGNt+q5cQwjHx7YtSXX36pDRs2aNq0aapUqVK2Q8DLlSunhg0b6s0339Rnn32mCRMm6Ouvv87239X1IiMjtWjRIr388ss2T8/LnNJkZ2enmJgYPfPMMxo2bJg+++wz/fnnn+Z0LUn63//+p1mzZundd9+Vo6PjLc8n8+/37NmzuTp/AAWjZMmS5rSO2/mc7927t65cuaJPP/3U3NemTZuUlJSk3r173/SYbm5u+v3333NccsEwDH3yySfq2rWrDMMw+3H27FkFBgYqOTk5V5/R2Tl37pyKFi2aZernzezdu1enT5/WP//5T5sH3wQFBalWrVq5yjeSbX7/888/lZycrEceeSRX5/Lrr7/e1lMCM50+fVrPPPOMfHx8NHr0aHN75uf9uXPntHDhQv3rX//SU089pfXr18vPz09vvPGGGTto0CDZ29vrqaee0o4dO3Ts2DFNmTLFHFXy999/Z3vsjIwMLV++XA0bNswywiTz+J6enlq/fr2eeuop/etf/9KHH36oY8eOZZniLF3LI+QQ4N6yatUqubq66rHHHrP5LM+cgp853dLNzU2StG7dulvO9skt7m0K7t7myJEjCgkJUUBAgIKDg83t1+eatWvXatiwYXrmmWcUExOjMmXK2OSaSpUqKSoqSkOHDlXXrl310ksvaf/+/SpXrpxefvnlbI+bm/ul6/3111/q1auXnJ2d9fbbb2cbQ64pOBTnUGj84x//yDJXX7o2vPrJJ5+Uq6urXFxcVK5cOT377LOSrj1J81YqVapk83NmYrhxXYcbTZo0SUlJSapRo4bq1q2rUaNG6YcffjDbf/75ZxmGoddee03lypWzeb3++uuSlO0Td3LLuM21yH777TdJUs2aNW22Ozg4qEqVKmb7rSxdulT16tUz16IoV66c1q9fn6trnZ/atGmjTp06KSwsTKtWrdLEiRNtphr98ssvatOmjQYMGKBXX31VTzzxhF5//XXNnTtXq1ev1pdffpntfr/++msNHDhQgYGBWdaJy7xx7Nq1q82Na/PmzeXj42NOB5Okl156SQ8//LB69OiRq/PJ/Pu9fv0lANa7ePGi+WTn2/mcr1+/vmrVqqUVK1aY+1qxYoXKli17y6cyjxkzRiVLllTTpk1VvXp1hYSE2EwhOXPmjJKSkrRgwYIs/cj88uFu8s3tyinfSFKtWrVynW/WrVun5s2by8nJSaVLlzanaeVXvrl06ZK6dOmiCxcuaO3atTaf65mf98WKFVPPnj3N7UWKFFHv3r31+++/m+sO1atXT5GRkTp27JhatGihatWqafbs2eY0qpwKndu2bdMff/yhvn37ZmnLPP5TTz1lcxPaq1cvFS1a1CbfZDIMgxwC3GN++uknJScny93dPcvn+cWLF83P8kcffVQ9evTQxIkTVbZsWT3xxBNasmTJLdc+vhnubQrm3iYhIUFBQUFydXXV6tWrZW9vb7Zlftb7+PioWbNm5vaSJUuqa9eu2r17t65evZrjvkuXLq3+/fvr6NGj+v3337O03+p+6Xrp6enq06ePDh8+rNWrV5tPdL0RuabgFLW6A0Cm7NZvSUpK0qOPPioXFxdNmjRJVatWlZOTk/bt26cxY8YoIyPjlvu9/gPxerdKEK1atdKxY8e0du1abdq0SQsXLtSMGTM0f/58DRo0yDz2v/71rxzXEqpWrdot+5edMmXK3LJ4mB8+/vhjPf/88+rWrZtGjRold3d3cz2h3Cy8XVCqVq2qhg0batmyZQoNDZV0bf2jy5cvq0uXLjaxmY/+/vbbb9WpUyebtu+//16PP/646tSpo9WrV6toUduPxMwkdeNDJiTJ3d3d/DvasmWLNm7cqE8//dRmJMfVq1f1999/69dff1Xp0qXl4uJitmW+98Y1jwBY5/fff1dycrL52X27n/O9e/fWm2++qbNnz6pUqVL6/PPP9fTTT2f5bLmRr6+vjh49qnXr1mnjxo365JNPNHfuXI0fP14TJ040+/Hss8/afAN/vXr16t32+UrX8s3Vq1d14cIFsyhZEL7++ms9/vjjatWqlebOnavy5curWLFiWrJkSbajxO5WWlqaunfvrh9++EFRUVGqU6eOTXvmwudubm5Zfm9wd3eXdO1zO/MLv549e+rxxx/X999/r/T0dDVq1MgcXVGjRo1s+7Bs2TIVKVJETz/9dJa2nPKNvb19jr8T/PnnnzajvQEUfhkZGXJ3d9eyZcuybc98yIOdnZ1Wr16tXbt26YsvvlBUVJQGDBigadOmadeuXbc12jkT9zb5f2+TnJysTp06KSkpSV9//XWWgtet7i2uXLmiS5cuydXVNcdjVKxYUdK1kY4VKlTIMS67+6XrDR48WOvWrdOyZctu+iXin3/+yf1KAaE4h0Jt69atOnfunD799FObBa3j4+ML5PiZ3070799fFy9eVKtWrTRhwgQNGjRIVapUkXTtW/ZbPWzgdr9tqFWrlpYtW6bk5OSbfjhfz9vbW5J09OhRs2/StRuS+Ph4mz7m1J/Vq1erSpUq+vTTT21iMr8tK0z+/vtvm28PExMTZRhGlqf4Zk4FuPFbqGPHjqljx45yd3fXhg0bsv0lx9/fX5KyHcJ/8uRJ1apVS5LM0RTZLcL7xx9/yMfHRzNmzNCIESPM7Zn/hm+2eC6AgpX5sILMm5Lb+ZyXrhXnJk6cqE8++UQeHh5KSUlRnz59cnXsEiVKqHfv3urdu7dZSHrzzTc1duxY8+EE6enp+ZJvpGufSbkt8F2fb278hf7o0aNm+83688knn8jJyUlRUVE2SwEsWbLktvqfGxkZGerXr59iYmK0cuVKPfroo1liihQpogYNGmjPnj1KS0uzGcl/8uRJSf9305zJwcHBfDqu9H8Pdcju7yg1NVWffPKJWrdune3ohJzyTVpams6ePZvl2FevXtWJEyfML6AA3BuqVq2qzZs3q0WLFtkOTLhR8+bN1bx5c7355puKjIxU3759tXz5cg0aNOiORjNxb3NNftzbXL58WV27dtX//vc/bd68WX5+fllivLy85OnpmeO9hZOT0y2/KMtcmunGvJCdG++XMo0aNUpLlizRzJkzs/3C6Hrx8fGqX7/+LY+Fu8e0VhRqmd9eXz/KLS0tTXPnzs33Y587d87m55IlS6patWrmB5y7u7tat26tDz74QKdOncry/jNnzph/LlGihCTbR1nfTEBAgAzDuOkT327Uvn17OTg4aPbs2TbXa9GiRUpOTlZQUJBNf7Ibyp3d9f7uu++0c+fOXPUht48bz62rV69m+y3b7t27deDAATVu3NjcVqNGDRmGkeVJR//9738lyebpUAkJCerQoYOKFCmiqKioHJNbzZo1Vb9+fa1du9ZmrYVNmzbpxIkT5lMI27ZtqzVr1mR5lStXTo0bN9aaNWvUtWtXm33HxsbK1dVVtWvXvs2rAiA/bNmyRZMnT5aPj4857fB2Puela8X2unXrasWKFVqxYoXKly9/0yelZrox3zg4OMjPz0+GYejKlSuyt7dXjx499Mknn+jgwYM37ced5Bvp2jpyudW4cWO5u7tr/vz5Nr/0f/nll/rxxx+z5Jvs+mNvby87OzubL1R+/fVXffbZZ7nqw7Fjx3I96mH48OFasWKF5s6de9MnGfbu3Vvp6elaunSpue3y5ctatmyZ/Pz8cpzyI12bqjZ//nx16dIl25FzGzZsUFJSUrZTWiWpdevW5miay5cvm9sjIiKUnp5u5ptMhw8f1uXLl/Xwww/n2CcAhc9TTz2l9PR0TZ48OUvb1atXzc/KP//8M8ssnwYNGkiS+bmb+WTN3H7ec29zTX7c26Snp6t3797auXOnVq1aZebW7PTu3VsnTpxQdHS0ue3s2bNau3at2rZtay5tcOPvGNK1L3AWL16sevXqqXz58pJu735Jkt5991299957evXVV/XSSy/d9LySk5N17Ngxck0BYeQcCrWHH35YDz30kIKDg/Xiiy/Kzs5OH3300W2vWXAn/Pz81Lp1a/n7+6t06dLau3evVq9ebTMsODw8XC1btlTdunU1ePBgValSRYmJidq5c6d+//13ff/995KuJVN7e3u98847Sk5OlqOjo9q2bWtOlblRy5YtVaZMGW3evPmWaxVlKleunMaOHauJEyeqY8eOevzxx3X06FHNnTtXTZo0Mdfpk659Q79ixQqFhYWpSZMm5joHXbp00aeffqonn3xSQUFBio+P1/z58+Xn52cuYHozvr6+evTRR3O1cOqcOXOUlJRkjkj44osvzLUThg8fLldXV128eFEVK1ZU7969Vbt2bZUoUUIHDhzQkiVL5Orqqtdee83c3/PPP6/33ntPL7zwgvbv36/atWtr3759WrhwoWrXrq0nn3zSjO3YsaN++eUXjR49Wt98842++eYbs83Dw8PmJmjGjBl67LHH1LJlS73wwgtKTk7W9OnTVaNGDQ0bNkzStXUNb1zbUJJGjBghDw8PdevWLUtbdHS0unbtyhoOgAW+/PJLHTlyRFevXlViYqK2bNmi6OhoeXt76/PPP7d5yEFuP+cz9e7dW+PHj5eTk5MGDhxos35YTjp06CBPT0+1aNFCHh4e+vHHHzVnzhwFBQWZ36C//fbb+uqrr9SsWTMNHjxYfn5+On/+vPbt26fNmzfr/Pnzkq6NynBzc9P8+fNVqlQplShRQs2aNZOPj0+2x65SpYrq1KmjzZs3a8CAAbm6fsWKFdM777yj/v3769FHH9XTTz+txMREzZo1S5UrV9bIkSPN2MwRYS+++KICAwNlb2+vPn36KCgoSNOnT1fHjh31zDPP6PTp0woPD1e1atVs1kDKSbt27STplg+FmDlzpubOnauAgAAVL15cH3/8sU37k08+ad5kvvDCC1q4cKFCQkL0v//9T5UqVdJHH32k3377TV988YXN+/z8/NSrVy9VqlRJ8fHxmjdvnkqXLq358+dn249ly5bJ0dExx3VJHR0d9e677yo4OFitWrXSc889p+PHj2vWrFl65JFHshQVo6OjVbx48SxFOwCF26OPPqoXXnhBU6ZMUVxcnDp06KBixYrpp59+0qpVqzRr1iz17NlTS5cu1dy5c/Xkk0+qatWqunDhgj788EO5uLioc+fOkq4tCeTn56cVK1aoRo0aKl26tOrUqZNl2n4m7m3y797m5Zdf1ueff66uXbvq/PnzWXLN9X0dO3asVq5cqR49eigsLEyurq6aP3++rly5orfeesuMGz16tI4dO6Z27drJy8tLv/76qz744ANdunRJs2bNMuNu535pzZo1Gj16tKpXry5fX98s/Xzsscdsptxu3rxZhmHoiSeeuOW1Qh4oqMfCAplufKS4YRjGo48+atSuXTvb+G+//dZo3ry54ezsbHh5eRmjR482oqKisjy+Ozg42OYR2vHx8YYk4913382yT+XiseNvvPGG0bRpU8PNzc1wdnY2atWqZbz55ptGWlqaTdyxY8eMfv36GZ6enkaxYsWMf/zjH0aXLl2M1atX28R9+OGHRpUqVQx7e/tcPXr8xRdfNKpVq5Zje06PMJ8zZ45Rq1Yto1ixYoaHh4cxbNgw488//7SJuXjxovHMM88Ybm5uhiTzumVkZBhvvfWW4e3tbTg6OhoNGzY01q1bl+XaGsbdP27c29vbfNT3ja/4+HjDMAwjNTXVeOmll4x69eoZLi4uRrFixQxvb29j4MCBZsz1fv/9d2PAgAGGj4+P4eDgYJQvX94YPHiwcebMmSz9zOmVXf+jo6ON5s2bG05OTkbp0qWN5557zjh16lSuzjG7R9b/+OOPhiRj8+bNubpWAPLGkiVLbP6/Ozg4GJ6ensZjjz1mzJo1y0hJScn2fbn9nDcMw/jpp5/M/X/zzTe56tcHH3xgtGrVyihTpozh6OhoVK1a1Rg1apSRnJxsE5eYmGiEhIQYFStWNIoVK2Z4enoa7dq1MxYsWGATt3btWsPPz88oWrSoIclYsmTJTY8/ffp0o2TJksZff/2VbfuePXuy3c+KFSuMhg0bGo6Ojkbp0qWNvn37Gr///rtNzNWrV43hw4cb5cqVM+zs7Gzy/6JFi4zq1asbjo6ORq1atYwlS5YYr7/+epbfEby9vY3g4OAs227MS9kJDg6+6Wf+jbkkMTHRCA4ONkqXLm04OjoazZo1MzZu3Jhlv3369DEqVqxoODg4GF5eXsbQoUONxMTEbPuQnJxsODk5Gd27d79lf//73/8a9evXNxwdHQ0PDw8jNDQ023+XzZo1M5599tlb7g+AtbK77zEMw1iwYIHh7+9vODs7G6VKlTLq1q1rjB492jh58qRhGIaxb98+4+mnnzYqVapkODo6Gu7u7kaXLl2MvXv32uxnx44dhr+/v+Hg4HDL+xvubfLv3ubRRx+9aa650bFjx4wnn3zScHFxMZydnY22bdsau3fvtomJjIw0WrVqZZQrV84oWrSoUbZsWePJJ580YmNjbeJu534pM8fm9Lrx2vfu3dto2bLlLc8fecPOMApgCBKA2/bLL7+oVq1a+vLLL80RArg/jBgxQtu3b1dsbCwj5wBYLjk5WVWqVNHUqVM1cOBAq7uDW4iLi1OjRo20b98+c5obABR23NvcWxISEuTj46Ply5czcq6AUJwDCrFhw4bp559/tlmTAPe2c+fOydvbWytXrjSnJQCA1d555x0tWbJEhw8fztVUXFinT58+ysjIyLLGKgAUdtzb3DteeeUVbdmyRbt377a6Kw8MinMAAAAAAACARfhqFAAAAAAAALAIxTkAAAAAAADAIhTnAAAAACCfVK5cWXZ2dlleISEhkqTLly8rJCREZcqUUcmSJdWjRw8lJiba7OP48eMKCgpS8eLF5e7urlGjRunq1as2MVu3blWjRo3k6OioatWqKSIioqBOEQBwlyjOAQAAAEA+2bNnj06dOmW+MhfD79WrlyRp5MiR+uKLL7Rq1Spt27ZNJ0+eVPfu3c33p6enKygoSGlpadqxY4eWLl2qiIgIjR8/3oyJj49XUFCQ2rRpo7i4OI0YMUKDBg1SVFRUwZ4sAOCO8ECIPJKRkaGTJ0+qVKlSsrOzs7o7AHDPMwxDFy5ckJeXF0+PFHkGAPKaVXlmxIgRWrdunX766SelpKSoXLlyioyMVM+ePSVJR44cka+vr3bu3KnmzZvryy+/VJcuXXTy5El5eHhIkubPn68xY8bozJkzcnBw0JgxY7R+/XodPHjQPE6fPn2UlJSkjRs35rpv5BoAyFu5zTVFC7BP97WTJ0+qYsWKVncDAO47J06cUIUKFazuhuXIMwCQPwoyz6Slpenjjz9WWFiY7OzsFBsbqytXrqh9+/ZmTK1atVSpUiWzOLdz507VrVvXLMxJUmBgoIYNG6ZDhw6pYcOG2rlzp80+MmNGjBhx0/6kpqYqNTXV/PmPP/6Qn59f3pwsAMB0q1xDcS6PlCpVStK1C+7i4mJxbwDg3peSkqKKFSuan68POvIMAOQtK/LMZ599pqSkJD3//POSpISEBDk4OMjNzc0mzsPDQwkJCWbM9YW5zPbMtpvFpKSk6O+//5azs3O2/ZkyZYomTpyYZTu5BgDyRm5zDcW5PJI57NvFxYVEBgB5iGk115BnACB/FGSeWbRokTp16iQvL68CO+bNjB07VmFhYebPmTeR5BoAyFu3yjUU5wAAAAAgn/3222/avHmzPv30U3Obp6en0tLSlJSUZDN6LjExUZ6enmbM7t27bfaV+TTX62NufMJrYmKiXFxcchw1J0mOjo5ydHS8q/MCANw9VtgGADzQKleuLDs7uyyvkJAQSdLly5cVEhKiMmXKqGTJkurRo0eWG6Djx48rKChIxYsXl7u7u0aNGqWrV6/axGzdulWNGjWSo6OjqlWrpoiIiII6RQBAIbBkyRK5u7srKCjI3Obv769ixYopJibG3Hb06FEdP35cAQEBkqSAgAAdOHBAp0+fNmOio6Pl4uJirg8XEBBgs4/MmMx9AAAKN4pzAIAH2p49e3Tq1CnzFR0dLUnq1auXJGnkyJH64osvtGrVKm3btk0nT55U9+7dzfenp6crKChIaWlp2rFjh5YuXaqIiAiNHz/ejImPj1dQUJDatGmjuLg4jRgxQoMGDVJUVFTBniwAwBIZGRlasmSJgoODVbTo/01ecnV11cCBAxUWFqavvvpKsbGx6t+/vwICAtS8eXNJUocOHeTn56fnnntO33//vaKiojRu3DiFhISYo96GDh2qX375RaNHj9aRI0c0d+5crVy5UiNHjrTkfAEAt8fOMAzD6k7cD1JSUuTq6qrk5GTWZwCAPGDV5+qIESO0bt06/fTTT0pJSVG5cuUUGRmpnj17SpKOHDkiX19f8yl6X375pbp06aKTJ0+ai3HPnz9fY8aM0ZkzZ+Tg4KAxY8Zo/fr1OnjwoHmcPn36KCkpSRs3bsxVv8gzAJC3CvJzddOmTQoMDNTRo0dVo0YNm7bLly/r5Zdf1n//+1+lpqYqMDBQc+fONaesStemxA4bNkxbt25ViRIlFBwcrLffftum0Ld161aNHDlShw8fVoUKFfTaa6+ZD57ILXINAOSt3H6usuYcAAD/X1pamj7++GOFhYXJzs5OsbGxunLlitq3b2/G1KpVS5UqVTKLczt37lTdunVtnpIXGBioYcOG6dChQ2rYsKF27txps4/MmBEjRuTYl9TUVKWmppo/p6Sk5N2JAgAKVIcOHZTTmAgnJyeFh4crPDw8x/d7e3trw4YNNz1G69attX///rvqJwDAGkxrBQDg//vss8+UlJRkjjRISEiQg4ODzSLdkuTh4aGEhAQz5vrCXGZ7ZtvNYlJSUvT3339n25cpU6bI1dXVfFWsWPFuTw8AAABAIWRpcW7evHmqV6+e+ajugIAAffnll2Z769atsyzQPXToUJt95NUi3OHh4apcubKcnJzUrFmzLE9EAgDc/xYtWqROnTrJy8vL6q5o7NixSk5ONl8nTpywuksAAAAA8oGlxbkKFSro7bffVmxsrPbu3au2bdvqiSee0KFDh8yYwYMH2yzUPXXqVLMtrxbhXrFihcLCwvT6669r3759ql+/vgIDA22eiAQAuL/99ttv2rx5swYNGmRu8/T0VFpampKSkmxiExMTzbWAPD09szy9NfPnW8W4uLjI2dk52/44OjqaX15lvgAAAADcfywtznXt2lWdO3dW9erVVaNGDb355psqWbKkdu3aZcYUL15cnp6e5uv6m5NNmzbp8OHD+vjjj9WgQQN16tRJkydPVnh4uNLS0iRdW5Tbx8dH06ZNk6+vr0JDQ9WzZ0/NmDHD3M/06dM1ePBg9e/fX35+fpo/f76KFy+uxYsXF9zFAABYasmSJXJ3d1dQUJC5zd/fX8WKFVNMTIy57ejRozp+/LgCAgIkSQEBATpw4IDNFzrR0dFycXGRn5+fGXP9PjJjMvcBAAAA4MFVaNacS09P1/Lly3Xp0iWbm5Vly5apbNmyqlOnjsaOHau//vrLbMtpEe6UlBRz9F1Oi3Dv3LlT0rXFv2NjY21iihQpovbt25sx2UlNTVVKSorNCwBwb8rIyNCSJUsUHBxs8+Q7V1dXDRw4UGFhYfrqq68UGxur/v37KyAgQM2bN5d0bZFvPz8/Pffcc/r+++8VFRWlcePGKSQkRI6OjpKkoUOH6pdfftHo0aN15MgRzZ07VytXrtTIkSMtOV8AAAAAhYflT2s9cOCAAgICdPnyZZUsWVJr1qwxRxo888wz8vb2lpeXl3744QeNGTNGR48e1aeffiopbxbh/vPPP5Wenp5tzJEjR3Ls95QpUzRx4sS7O3kAQKGwefNmHT9+XAMGDMjSNmPGDBUpUkQ9evRQamqqAgMDNXfuXLPd3t5e69at07BhwxQQEKASJUooODhYkyZNMmN8fHy0fv16jRw5UrNmzVKFChW0cOFCBQYGFsj5AQAAACi8LC/O1axZU3FxcUpOTtbq1asVHBysbdu2yc/PT0OGDDHj6tatq/Lly6tdu3Y6duyYqlatamGvry3UHRYWZv6ckpLCk/QA2Kj8ynqru1Ao/Pp20K2DLNahQwcZhpFtm5OTk8LDwxUeHp7j+729vbVhw4abHqN169bav3//XfUThQP/t++N/9cAcK8iz1xDrsGDxPLinIODg6pVqybp2to+e/bs0axZs/TBBx9kiW3WrJkk6eeff1bVqlXl6emZ5amqt7sIt729vezt7bONydxHdhwdHc3pSnmFD2E+gAEAAAAAd4/7a+6v7yWFZs25TBkZGUpNTc22LS4uTpJUvnx5SXmzCLeDg4P8/f1tYjIyMhQTE8NC3QAAAAAAAMhXlo6cGzt2rDp16qRKlSrpwoULioyM1NatWxUVFaVjx44pMjJSnTt3VpkyZfTDDz9o5MiRatWqlerVqyfJdhHuqVOnKiEhIdtFuOfMmaPRo0drwIAB2rJli1auXKn16/+vih4WFqbg4GA1btxYTZs21cyZM3Xp0iX179/fkusCAAAA3AqjQq5hZAgA4F5naXHu9OnT6tevn06dOiVXV1fVq1dPUVFReuyxx3TixAlt3rzZLJRVrFhRPXr00Lhx48z359Ui3L1799aZM2c0fvx4JSQkqEGDBtq4cWOWh0QAAAAAAAAAecnS4tyiRYtybKtYsaK2bdt2y33k1SLcoaGhCg0NveXxULjxDfI1fIMMAAAAAMC9wfIHQgAofChyXkOREwAAIPf4HfIafofE/YT/1wXzf5riHAAAAAocv+xzAw8AAK4pdE9rBQAAAAAAAB4UFOcAAAAAAAAAizCtFQCABwTTCJlGCAAAgMKHkXMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hwAAAAAAABgEYpzAAAAAAAAgEUozgEAAAAAAAAWoTgHAAAAAAAAWITiHAAAAAAAAGARinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAACQj/744w89++yzKlOmjJydnVW3bl3t3bvXbDcMQ+PHj1f58uXl7Oys9u3b66effrLZx/nz59W3b1+5uLjIzc1NAwcO1MWLF21ifvjhBz3yyCNycnJSxYoVNXXq1AI5PwDA3aE4BwAAAAD55M8//1SLFi1UrFgxffnllzp8+LCmTZumhx56yIyZOnWqZs+erfnz5+u7775TiRIlFBgYqMuXL5sxffv21aFDhxQdHa1169Zp+/btGjJkiNmekpKiDh06yNvbW7GxsXr33Xc1YcIELViwoEDPFwBw+4pa3QEAAAAAuF+98847qlixopYsWWJu8/HxMf9sGIZmzpypcePG6YknnpAk/ec//5GHh4c+++wz9enTRz/++KM2btyoPXv2qHHjxpKk999/X507d9Z7770nLy8vLVu2TGlpaVq8eLEcHBxUu3ZtxcXFafr06TZFPABA4cPIOQAAAADIJ59//rkaN26sXr16yd3dXQ0bNtSHH35otsfHxyshIUHt27c3t7m6uqpZs2bauXOnJGnnzp1yc3MzC3OS1L59exUpUkTfffedGdOqVSs5ODiYMYGBgTp69Kj+/PPPbPuWmpqqlJQUmxcAoOBRnAMAAACAfPLLL79o3rx5ql69uqKiojRs2DC9+OKLWrp0qSQpISFBkuTh4WHzPg8PD7MtISFB7u7uNu1FixZV6dKlbWKy28f1x7jRlClT5Orqar4qVqx4l2cLALgTFOcAAAAAIJ9kZGSoUaNGeuutt9SwYUMNGTJEgwcP1vz5863umsaOHavk5GTzdeLECau7BAAPJIpzAIAHHk/RAwDkl/Lly8vPz89mm6+vr44fPy5J8vT0lCQlJibaxCQmJpptnp6eOn36tE371atXdf78eZuY7PZx/TFu5OjoKBcXF5sXAKDgUZwDADzQeIoeACA/tWjRQkePHrXZ9r///U/e3t6Srj0cwtPTUzExMWZ7SkqKvvvuOwUEBEiSAgIClJSUpNjYWDNmy5YtysjIULNmzcyY7du368qVK2ZMdHS0atasaZPTAACFD09rBQA80HiKHgAgP40cOVIPP/yw3nrrLT311FPavXu3FixYYH45Y2dnpxEjRuiNN95Q9erV5ePjo9dee01eXl7q1q2bpGsj7Tp27GhOh71y5YpCQ0PVp08feXl5SZKeeeYZTZw4UQMHDtSYMWN08OBBzZo1SzNmzLDq1AEAucTIOQDAA62wPkWPJ+gBwP2hSZMmWrNmjf773/+qTp06mjx5smbOnKm+ffuaMaNHj9bw4cM1ZMgQNWnSRBcvXtTGjRvl5ORkxixbtky1atVSu3bt1LlzZ7Vs2dJm9LWrq6s2bdqk+Ph4+fv76+WXX9b48eP5AggA7gGMnAMAPNAyn6IXFhamV199VXv27NGLL74oBwcHBQcH5+lT9K4fkXf9PhMSErJMOZoyZYomTpyYdycKALBMly5d1KVLlxzb7ezsNGnSJE2aNCnHmNKlSysyMvKmx6lXr56+/vrrO+4nAMAalo6cmzdvnurVq2cuPhoQEKAvv/zSbL98+bJCQkJUpkwZlSxZUj169MiyyOnx48cVFBSk4sWLy93dXaNGjdLVq1dtYrZu3apGjRrJ0dFR1apVU0RERJa+hIeHq3LlynJyclKzZs20e/fufDlnAEDhUlifoscT9AAAAIAHg6XFuQoVKujtt99WbGys9u7dq7Zt2+qJJ57QoUOHJF1bn+GLL77QqlWrtG3bNp08eVLdu3c335+enq6goCClpaVpx44dWrp0qSIiIjR+/HgzJj4+XkFBQWrTpo3i4uI0YsQIDRo0SFFRUWbMihUrFBYWptdff1379u1T/fr1FRgYmOWJSACA+09hfYoeT9ADAAAAHgyWFue6du2qzp07q3r16qpRo4befPNNlSxZUrt27VJycrIWLVqk6dOnq23btvL399eSJUu0Y8cO7dq1S5K0adMmHT58WB9//LEaNGigTp06afLkyQoPD1daWpokaf78+fLx8dG0adPk6+ur0NBQ9ezZ02Zh1OnTp2vw4MHq37+//Pz8NH/+fBUvXlyLFy/Ose+sBQQA9weeogcAAADASoXmgRDp6elavny5Ll26pICAAMXGxurKlSs2C3DXqlVLlSpVslmAu27dujbrAAUGBiolJcUcfbdz506bfWTGZO4jLS1NsbGxNjFFihRR+/btzZjsTJkyRa6uruarYsWKd38RAAAFbuTIkdq1a5feeust/fzzz4qMjNSCBQsUEhIiyfYpep9//rkOHDigfv365fgUvd27d+vbb7/N9il6Dg4OGjhwoA4dOqQVK1Zo1qxZCgsLs+rUAQAAABQClhfnDhw4oJIlS8rR0VFDhw7VmjVr5Ofnp4SEBDk4OMjNzc0m/sYFuLNboDuz7WYxKSkp+vvvv3X27Fmlp6ffdKHv7LAWEADcH3iKHgAAAAArWf601po1ayouLk7JyclavXq1goODtW3bNqu7dUuOjo5ydHS0uhsAgDzAU/QAAAAAWMXy4pyDg4OqVasmSfL399eePXs0a9Ys9e7dW2lpaUpKSrIZPXfjAtw3PlX1xsW1c1qA28XFRc7OzrK3t5e9vf1NF/oGAAAAAAAA8oPl01pvlJGRodTUVPn7+6tYsWI2C3AfPXpUx48ft1mA+8CBAzZPyIuOjpaLi4v55L2AgACbfWTGZO7DwcFB/v7+NjEZGRmKiYkxYwAAAAAAAID8YOnIubFjx6pTp06qVKmSLly4oMjISG3dulVRUVFydXXVwIEDFRYWptKlS8vFxUXDhw9XQECAmjdvLknq0KGD/Pz89Nxzz2nq1KlKSEjQuHHjFBISYk45HTp0qObMmaPRo0drwIAB2rJli1auXKn169eb/QgLC1NwcLAaN26spk2baubMmbp06ZL69+9vyXUBAAAAAADAg8HS4tzp06fVr18/nTp1Sq6urqpXr56ioqL02GOPSZJmzJihIkWKqEePHkpNTVVgYKDmzp1rvt/e3l7r1q3TsGHDFBAQoBIlSig4ONhmTSAfHx+tX79eI0eO1KxZs1ShQgUtXLhQgYGBZkzv3r115swZjR8/XgkJCWrQoIE2btyY5SERAAAAAAAAQF6ytDi3aNGim7Y7OTkpPDxc4eHhOcZ4e3trw4YNN91P69attX///pvGhIaGKjQ09KYxAAAAAAAAQF4qdGvOAQAAAAAAAA8KinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hwAAAAAAABgEYpzAAAAAAAAgEUozgEAAAAAAAAWoTgHAAAAAAAAWITiHAAAAAAAAGARinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hwAAAAAAABgEYpzAAAAAAAAgEUozgEAAABAPpkwYYLs7OxsXrVq1TLbL1++rJCQEJUpU0YlS5ZUjx49lJiYaLOP48ePKygoSMWLF5e7u7tGjRqlq1ev2sRs3bpVjRo1kqOjo6pVq6aIiIiCOD0AQB6gOAcAAAAA+ah27do6deqU+frmm2/MtpEjR+qLL77QqlWrtG3bNp08eVLdu3c329PT0xUUFKS0tDTt2LFDS5cuVUREhMaPH2/GxMfHKygoSG3atFFcXJxGjBihQYMGKSoqqkDPEwBwZ4pa3QEAAAAAuJ8VLVpUnp6eWbYnJydr0aJFioyMVNu2bSVJS5Yska+vr3bt2qXmzZtr06ZNOnz4sDZv3iwPDw81aNBAkydP1pgxYzRhwgQ5ODho/vz58vHx0bRp0yRJvr6++uabbzRjxgwFBgYW6LkCAG4fI+cAAAAAIB/99NNP8vLyUpUqVdS3b18dP35ckhQbG6srV66offv2ZmytWrVUqVIl7dy5U5K0c+dO1a1bVx4eHmZMYGCgUlJSdOjQITPm+n1kxmTuIyepqalKSUmxeQEACh7FOQAAAADIJ82aNVNERIQ2btyoefPmKT4+Xo888oguXLighIQEOTg4yM3NzeY9Hh4eSkhIkCQlJCTYFOYy2zPbbhaTkpKiv//+O8e+TZkyRa6uruarYsWKd3u6AIA7QHEOAPBAY6FuAEB+6tSpk3r16qV69eopMDBQGzZsUFJSklauXGl11zR27FglJyebrxMnTljdJQB4IFGcAwA88FioGwBQUNzc3FSjRg39/PPP8vT0VFpampKSkmxiEhMTzTXqPD09s3wplPnzrWJcXFzk7OycY18cHR3l4uJi8wIAFDyKcwCAB17mQt2Zr7Jly0r6v4W6p0+frrZt28rf319LlizRjh07tGvXLkkyF+r++OOP1aBBA3Xq1EmTJ09WeHi40tLSJMlmoW5fX1+FhoaqZ8+emjFjhmXnDACwxsWLF3Xs2DGVL19e/v7+KlasmGJiYsz2o0eP6vjx4woICJAkBQQE6MCBAzp9+rQZEx0dLRcXF/n5+Zkx1+8jMyZzHwCAwo3iHADggVcYF+pmkW4AuD/861//0rZt2/Trr79qx44devLJJ2Vvb6+nn35arq6uGjhwoMLCwvTVV18pNjZW/fv3V0BAgJo3by5J6tChg/z8/PTcc8/p+++/V1RUlMaNG6eQkBA5OjpKkoYOHapffvlFo0eP1pEjRzR37lytXLlSI0eOtPLUAQC5ZGlxbsqUKWrSpIlKlSold3d3devWTUePHrWJad26dZa1gIYOHWoTk1dr/YSHh6ty5cpycnJSs2bNtHv37jw/ZwBA4VJYF+pmkW4AuD/8/vvvevrpp1WzZk099dRTKlOmjHbt2qVy5cpJkmbMmKEuXbqoR48eatWqlTw9PfXpp5+a77e3t9e6detkb2+vgIAAPfvss+rXr58mTZpkxvj4+Gj9+vWKjo5W/fr1NW3aNC1cuFCBgYEFfr4AgNtX1MqDb9u2TSEhIWrSpImuXr2qV199VR06dNDhw4dVokQJM27w4ME2yad48eLmnzPX+vH09NSOHTt06tQp9evXT8WKFdNbb70l6f/W+hk6dKiWLVummJgYDRo0SOXLlzcT1ooVKxQWFqb58+erWbNmmjlzpgIDA3X06FG5u7sX0BUBABS0Tp06mX+uV6+emjVrJm9vb61cufKm6/Tkt7FjxyosLMz8OSUlhQIdANyDli9fftN2JycnhYeHKzw8PMcYb29vbdiw4ab7ad26tfbv339HfQQAWMvS4tzGjRttfo6IiJC7u7tiY2PVqlUrc3vx4sXNxU5vlLnWz+bNm+Xh4aEGDRpo8uTJGjNmjCZMmCAHBwebtX4kydfXV998841mzJhhFuemT5+uwYMHq3///pKurQ+0fv16LV68WK+88kqW46ampio1NdX8melGAHB/uH6h7scee8xcqPv60XM3LtR940jrvFio29HR0ZyuBAAAAOD+VajWnEtOTpYklS5d2mb7smXLVLZsWdWpU0djx47VX3/9ZbblxVo/aWlpio2NtYkpUqSI2rdvn+N6QEw3AoD7Ewt1AwAAAChIhaY4l5GRoREjRqhFixaqU6eOuf2ZZ57Rxx9/rK+++kpjx47VRx99pGeffdZsz4u1fs6ePav09PRsYzL3caOxY8cqOTnZfJ04ceLOTx4AYBkW6gYAAABgJUuntV4vJCREBw8e1DfffGOzfciQIeaf69atq/Lly6tdu3Y6duyYqlatWtDdNDHdCADuD5kLdZ87d07lypVTy5YtsyzUXaRIEfXo0UOpqakKDAzU3LlzzfdnLtQ9bNgwBQQEqESJEgoODs52oe6RI0dq1qxZqlChAgt1AwAAAJBUSIpzoaGhWrdunbZv364KFSrcNLZZs2aSpJ9//llVq1bNk7V+7O3tZW9vn21MTmvdAQDuDyzUDQAAAMBKlk5rNQxDoaGhWrNmjbZs2SIfH59bvicuLk6SVL58eUl5s9aPg4OD/P39bWIyMjIUExPDekAAAAAAAADIN5aOnAsJCVFkZKTWrl2rUqVKmeu7ubq6ytnZWceOHVNkZKQ6d+6sMmXK6IcfftDIkSPVqlUr1atXT5LtWj9Tp05VQkJCtmv9zJkzR6NHj9aAAQO0ZcsWrVy5UuvXrzf7EhYWpuDgYDVu3FhNmzbVzJkzdenSJfPprQAAAAAAAEBes7Q4N2/ePEnXpvpcb8mSJXr++efl4OCgzZs3m4WyihUrqkePHho3bpwZm1dr/fTu3VtnzpzR+PHjlZCQoAYNGmjjxo1ZHhIBAAAAAAAA5BVLi3OGYdy0vWLFitq2bdst95NXa/2EhoYqNDT0lscDAAAAAAAA8oKla84BAAAAAAAADzKKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hwAAAAAAABgEYpzAAAAAAAAgEUozgEAAAAAAAAWoTgHAAAAAAAAWITiHAAAAAAAAGARinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hwAAAAAAABgEYpzAAAAAAAAgEUozgEAAAAAAAAWoTgHAAAAAAAAWITiHAAAAAAAAGARinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAABSQt99+W3Z2dhoxYoS57fLlywoJCVGZMmVUsmRJ9ejRQ4mJiTbvO378uIKCglS8eHG5u7tr1KhRunr1qk3M1q1b1ahRIzk6OqpatWqKiIgogDMCANwtinMAAAAAUAD27NmjDz74QPXq1bPZPnLkSH3xxRdatWqVtm3bppMnT6p79+5me3p6uoKCgpSWlqYdO3Zo6dKlioiI0Pjx482Y+Ph4BQUFqU2bNoqLi9OIESM0aNAgRUVFFdj5AQDuDMU5AAAAAMhnFy9eVN++ffXhhx/qoYceMrcnJydr0aJFmj59utq2bSt/f38tWbJEO3bs0K5duyRJmzZt0uHDh/Xxxx+rQYMG6tSpkyZPnqzw8HClpaVJkubPny8fHx9NmzZNvr6+Cg0NVc+ePTVjxgxLzhcAkHsU5wAAAAAgn4WEhCgoKEjt27e32R4bG6srV67YbK9Vq5YqVaqknTt3SpJ27typunXrysPDw4wJDAxUSkqKDh06ZMbcuO/AwEBzH9lJTU1VSkqKzQsAUPAozgEA8P+xDhAAID8sX75c+/bt05QpU7K0JSQkyMHBQW5ubjbbPTw8lJCQYMZcX5jLbM9su1lMSkqK/v7772z7NWXKFLm6upqvihUr3tH5AQDuDsU5AADEOkAAgPxx4sQJvfTSS1q2bJmcnJys7o6NsWPHKjk52XydOHHC6i4BwAOJ4hwA4IHHOkAAgPwSGxur06dPq1GjRipatKiKFi2qbdu2afbs2SpatKg8PDyUlpampKQkm/clJibK09NTkuTp6Zll1Hbmz7eKcXFxkbOzc7Z9c3R0lIuLi80LAFDw7qg4V6VKFZ07dy7L9qSkJFWpUiXX+5kyZYqaNGmiUqVKyd3dXd26ddPRo0dtYgpyOlF4eLgqV64sJycnNWvWTLt37871uQAAClZe5SKJdYAAAFnlVZ5p166dDhw4oLi4OPPVuHFj9e3b1/xzsWLFFBMTY77n6NGjOn78uAICAiRJAQEBOnDggE6fPm3GREdHy8XFRX5+fmbM9fvIjMncBwCg8Lqj4tyvv/6q9PT0LNtTU1P1xx9/5Ho/27ZtU0hIiHbt2qXo6GhduXJFHTp00KVLl8yYgppOtGLFCoWFhen111/Xvn37VL9+fQUGBtokQABA4ZFXuYh1gAAA2cmrPFOqVCnVqVPH5lWiRAmVKVNGderUkaurqwYOHKiwsDB99dVXio2NVf/+/RUQEKDmzZtLkjp06CA/Pz8999xz+v777xUVFaVx48YpJCREjo6OkqShQ4fql19+0ejRo3XkyBHNnTtXK1eu1MiRI/PmggAA8k3R2wn+/PPPzT9HRUXJ1dXV/Dk9PV0xMTGqXLlyrve3ceNGm58jIiLk7u6u2NhYtWrVypxOFBkZqbZt20qSlixZIl9fX+3atUvNmzc3pxNt3rxZHh4eatCggSZPnqwxY8ZowoQJcnBwsJlOJEm+vr765ptvNGPGDAUGBkqSpk+frsGDB6t///6Srk1BWr9+vRYvXqxXXnnldi4TACAf5WUuylwHKDo6ulCuAxQWFmb+nJKSQoEOAApAXt/z5MaMGTNUpEgR9ejRQ6mpqQoMDNTcuXPNdnt7e61bt07Dhg1TQECASpQooeDgYE2aNMmM8fHx0fr16zVy5EjNmjVLFSpU0MKFC837HQBA4XVbxblu3bpJkuzs7BQcHGzTVqxYMVWuXNksgN2J5ORkSVLp0qUl3Xo6UfPmzXOcTjRs2DAdOnRIDRs2zHE6UebT+NLS0hQbG6uxY8ea7UWKFFH79u1znHKUmpqq1NRU82emGwFAwcjLXHT9OkCZ0tPTtX37ds2ZM0dRUVHmOkDXj567cR2gG5dByKt1gDJHQwAACk5+3/NI15bcuZ6Tk5PCw8MVHh6e43u8vb21YcOGm+63devW2r9//131DQBQ8G6rOJeRkSHp2rcye/bsUdmyZfOsIxkZGRoxYoRatGihOnXqSCq46UR//vmn0tPTs405cuRItv2dMmWKJk6ceGcnCwC4Y3mZizLXAbpe//79VatWLY0ZM0YVK1Y01wHq0aOHpOzXAXrzzTd1+vRpubu7S8p+HaAbb6hYBwgACqf8vOcBACA7t1WcyxQfH5/X/VBISIgOHjyob775Js/3nR+YbgQA1sqLXJS5DtD1rl8HSJK5DlDp0qXl4uKi4cOH57gO0NSpU5WQkJDtOkBz5szR6NGjNWDAAG3ZskUrV67U+vXr7/ocAAD5Iz/ueQAAyM4dFeckKSYmRjExMTp9+rT57VKmxYsX39a+QkNDtW7dOm3fvl0VKlQwt3t6ehbIdCJ7e3vZ29tnG5O5jxsx3QgArJeXuSgnrAMEAA+ugsgzAADcUXFu4sSJmjRpkho3bqzy5cvLzs7ujg5uGIaGDx+uNWvWaOvWrfLx8bFp9/f3L5DpRA4ODvL391dMTIy5xkRGRoZiYmIUGhp6R+cGAMhfeZWLbsQ6QAAAKf/yDAAAN7qj4tz8+fMVERGh55577q4OHhISosjISK1du1alSpUy14hzdXWVs7OzzWPF83s6UVhYmIKDg9W4cWM1bdpUM2fO1KVLl8yntwIACpe8ykUAAGSHPAMAKCh3VJxLS0vTww8/fNcHnzdvnqRrowmut2TJEj3//POSCm46Ue/evXXmzBmNHz9eCQkJatCggTZu3JjlIREAgMIhr3IRAADZIc8AAArKHRXnBg0apMjISL322mt3dXDDMG4ZU5DTiUJDQ5nGCgD3iLzKRQAAZIc8AwAoKHdUnLt8+bIWLFigzZs3q169eipWrJhN+/Tp0/OkcwAA5IRcBADIT+QZAEBBuaPi3A8//KAGDRpIkg4ePGjTxkKpAICCQC4CAOQn8gwAoKDcUXHuq6++yut+AABwW8hFAID8RJ4BABSUIlZ3AAAAAAAAAHhQ3dHIuTZt2tx0KPeWLVvuuEMAAOQGuQgAkJ/IMwCAgnJHxbnMtRcyXblyRXFxcTp48KCCg4Pzol8AANwUuQgAkJ/IMwCAgnJHxbkZM2Zku33ChAm6ePHiXXUIAIDcIBcBAPITeQYAUFDydM25Z599VosXL87LXQIAcFvIRQCA/ESeAQDktTwtzu3cuVNOTk55uUsAAG4LuQgAkJ/IMwCAvHZH01q7d+9u87NhGDp16pT27t2r1157LU86BgDAzZCLAAD5iTwDACgod1Scc3V1tfm5SJEiqlmzpiZNmqQOHTrkSccAALgZchEAID+RZwAABeWOinNLlizJ634AAHBbyEUAgPxEngEAFJQ7Ks5lio2N1Y8//ihJql27tho2bJgnnQIAILfIRQCA/ESeAQDktzsqzp0+fVp9+vTR1q1b5ebmJklKSkpSmzZttHz5cpUrVy4v+wgAQBbkIgBAfiLPAAAKyh09rXX48OG6cOGCDh06pPPnz+v8+fM6ePCgUlJS9OKLL+Z1HwEAyIJcBADIT+QZAEBBuaORcxs3btTmzZvl6+trbvPz81N4eDiLowIACgS5CACQn8gzAICCckcj5zIyMlSsWLEs24sVK6aMjIy77hQAALdCLgIA5CfyDACgoNxRca5t27Z66aWXdPLkSXPbH3/8oZEjR6pdu3Z51jkAAHJCLgIA5CfyDACgoNxRcW7OnDlKSUlR5cqVVbVqVVWtWlU+Pj5KSUnR+++/n9d9BAAgC3IRACA/kWcAAAXljtacq1ixovbt26fNmzfryJEjkiRfX1+1b98+TzsHAEBOyEUAgPxEngEAFJTbGjm3ZcsW+fn5KSUlRXZ2dnrsscc0fPhwDR8+XE2aNFHt2rX19ddf51dfAQAgFwEA8hV5BgBQ0G6rODdz5kwNHjxYLi4uWdpcXV31wgsvaPr06XnWOQAAbkQuAgDkJ/IMAKCg3VZx7vvvv1fHjh1zbO/QoYNiY2PvulMAAOSEXAQAyE/kGQBAQbut4lxiYmK2jxPPVLRoUZ05c+auOwUAQE7IRQCA/ESeAQAUtNsqzv3jH//QwYMHc2z/4YcfVL58+bvuFAAAOSEXAQDyE3kGAFDQbqs417lzZ7322mu6fPlylra///5br7/+urp06ZJnnQMA4EbkIgBAfiLPAAAK2m0V58aNG6fz58+rRo0amjp1qtauXau1a9fqnXfeUc2aNXX+/Hn9+9//zq++AgBALgIA5Ku8zjPz5s1TvXr15OLiIhcXFwUEBOjLL7802y9fvqyQkBCVKVNGJUuWVI8ePZSYmGizj+PHjysoKEjFixeXu7u7Ro0apatXr9rEbN26VY0aNZKjo6OqVaumiIiIu7oOAICCU/R2gj08PLRjxw4NGzZMY8eOlWEYkiQ7OzsFBgYqPDxcHh4e+dJRAAAkchEAIH/ldZ6pUKGC3n77bVWvXl2GYWjp0qV64okntH//ftWuXVsjR47U+vXrtWrVKrm6uio0NFTdu3fXt99+K0lKT09XUFCQPD09tWPHDp06dUr9+vVTsWLF9NZbb0mS4uPjFRQUpKFDh2rZsmWKiYnRoEGDVL58eQUGBub9RQIA5KnbGjknSd7e3tqwYYPOnj2r7777Trt27dLZs2e1YcMG+fj45EcfAQCwkZe5iBENAIAb5WWe6dq1qzp37qzq1aurRo0aevPNN1WyZEnt2rVLycnJWrRokaZPn662bdvK399fS5Ys0Y4dO7Rr1y5J0qZNm3T48GF9/PHHatCggTp16qTJkycrPDxcaWlpkqT58+fLx8dH06ZNk6+vr0JDQ9WzZ0/NmDHjpn1LTU1VSkqKzQsAUPBuuziX6aGHHlKTJk3UtGlTPfTQQ3nZJwAAciUvclHmiIbY2Fjt3btXbdu21RNPPKFDhw5JkkaOHKkvvvhCq1at0rZt23Ty5El1797dfH/miIa0tDTt2LFDS5cuVUREhMaPH2/GZI5oaNOmjeLi4jRixAgNGjRIUVFRd3cBAAD5Kq/vedLT07V8+XJdunRJAQEBio2N1ZUrV9S+fXszplatWqpUqZJ27twpSdq5c6fq1q1rM1ovMDBQKSkpZq7auXOnzT4yYzL3kZMpU6bI1dXVfFWsWPGuzxEAcPtua1orAAD3m65du9r8/Oabb2revHnatWuXKlSooEWLFikyMlJt27aVJC1ZskS+vr7atWuXmjdvbo5o2Lx5szw8PNSgQQNNnjxZY8aM0YQJE+Tg4GAzokGSfH199c0332jGjBlMNwKAB8CBAwcUEBCgy5cvq2TJklqzZo38/PwUFxcnBwcHubm52cR7eHgoISFBkpSQkJBlGm3mz7eKSUlJ0d9//y1nZ+ds+zV27FiFhYWZP6ekpFCgAwAL3PHIubywfft2de3aVV5eXrKzs9Nnn31m0/7888/Lzs7O5tWxY0ebmPPnz6tv375ycXGRm5ubBg4cqIsXL9rE/PDDD3rkkUfk5OSkihUraurUqVn6smrVKtWqVUtOTk6qW7euNmzYkOfnCwAo3ArTiAamGgHA/aNmzZqKi4vTd999p2HDhik4OFiHDx+2ultydHQ0l3XIfAEACp6lxblLly6pfv36Cg8PzzGmY8eOOnXqlPn673//a9Pet29fHTp0SNHR0Vq3bp22b9+uIUOGmO0pKSnq0KGDvL29FRsbq3fffVcTJkzQggULzJgdO3bo6aef1sCBA7V//35169ZN3bp108GDB/P+pAEAhc6BAwdUsmRJOTo6aujQoeaIhoSEhAIZ0ZAdphoBwP3DwcFB1apVk7+/v6ZMmaL69etr1qxZ8vT0VFpampKSkmziExMT5enpKUny9PTMstZp5s+3inFxcclx1BwAoPCwtDjXqVMnvfHGG3ryySdzjHF0dJSnp6f5un6thx9//FEbN27UwoUL1axZM7Vs2VLvv/++li9frpMnT0qSli1bprS0NC1evFi1a9dWnz599OKLL2r69OnmfmbNmqWOHTtq1KhR8vX11eTJk9WoUSPNmTMn/04eAFBoFMYRDWPHjlVycrL5OnHihKX9AQDknYyMDKWmpsrf31/FihVTTEyM2Xb06FEdP35cAQEBkqSAgAAdOHBAp0+fNmOio6Pl4uIiPz8/M+b6fWTGZO4DAFC4WVqcy42tW7fK3d1dNWvW1LBhw3Tu3DmzbefOnXJzc1Pjxo3Nbe3bt1eRIkX03XffmTGtWrWSg4ODGRMYGKijR4/qzz//NGOYbgQAD67COKKBqUYAcH8YO3astm/frl9//VUHDhzQ2LFjtXXrVvXt21eurq4aOHCgwsLC9NVXXyk2Nlb9+/dXQECAmjdvLknq0KGD/Pz89Nxzz+n7779XVFSUxo0bp5CQEDk6OkqShg4dql9++UWjR4/WkSNHNHfuXK1cuVIjR4608tQBALlUqItzHTt21H/+8x/FxMTonXfe0bZt29SpUyelp6dLujZNyN3d3eY9RYsWVenSpfNkulFme3aYbgQA9y9GNAAA8srp06fVr18/1axZU+3atdOePXsUFRWlxx57TJI0Y8YMdenSRT169FCrVq3k6empTz/91Hy/vb291q1bJ3t7ewUEBOjZZ59Vv379NGnSJDPGx8dH69evV3R0tOrXr69p06Zp4cKFPHQIAO4RhfpprX369DH/XLduXdWrV09Vq1bV1q1b1a5dOwt7xpONAOB+MXbsWHXq1EmVKlXShQsXFBkZqa1btyoqKspmREPp0qXl4uKi4cOH5ziiYerUqUpISMh2RMOcOXM0evRoDRgwQFu2bNHKlSu1fv16K08dAFAAFi1adNN2JycnhYeH33Qdbm9v71s+sK5169bav3//HfURAGCtQl2cu1GVKlVUtmxZ/fzzz2rXrp08PT1tRipI0tWrV3X+/Pk8mW6U2Z4dR0dH86YLAHDvyhzRcOrUKbm6uqpevXpZRjQUKVJEPXr0UGpqqgIDAzV37lzz/ZkjGoYNG6aAgACVKFFCwcHB2Y5oGDlypGbNmqUKFSowogEAAACApHusOPf777/r3LlzKl++vKRr04SSkpIUGxsrf39/SdKWLVuUkZGhZs2amTH//ve/deXKFRUrVkzStalENWvWNB8ukTndaMSIEeaxmG4EAA8GRjQAAAAAsJKla85dvHhRcXFxiouLkyTFx8crLi5Ox48f18WLFzVq1Cjt2rVLv/76q2JiYvTEE0+oWrVq5kgDX19fdezYUYMHD9bu3bv17bffKjQ0VH369JGXl5ck6ZlnnpGDg4MGDhyoQ4cOacWKFZo1a5bNlNSXXnpJGzdu1LRp03TkyBFNmDBBe/fuVWhoaIFfEwAAAAAAADw4LC3O7d27Vw0bNlTDhg0lSWFhYWrYsKHGjx8ve3t7/fDDD3r88cdVo0YNDRw4UP7+/vr6669tppMuW7ZMtWrVUrt27dS5c2e1bNlSCxYsMNtdXV21adMmxcfHy9/fXy+//LLGjx+vIUOGmDEPP/ywIiMjtWDBAtWvX1+rV6/WZ599pjp16hTcxQAAAAAAAMADx9Jpra1bt5ZhGDm2R0VF3XIfpUuXVmRk5E1j6tWrp6+//vqmMb169VKvXr1ueTwAAAAAAAAgr1g6cg4AAAAAAAB4kFGcAwAAAAAAACxCcQ4AAAAAAACwCMU5AAAAAAAAwCIU5wAAAAAAAACLUJwDAAAAAAAALEJxDgAAAAAAALAIxTkAAAAAAADAIhTnAAAAAAAAAItQnAMAAAAAAAAsQnEOAAAAAAAAsAjFOQAAAAAAAMAiFOcAAAAAAAAAi1CcAwAAAAAAACxCcQ4AAAAAAACwCMU5AAAAAAAAwCIU5wAAAAAAAACLUJwDAAAAAAAALEJxDgAAAAAAALAIxTkAAAAAAADAIhTnAAAAAAAAAItQnAMAAAAAAAAsQnEOAAAAAAAAsAjFOQAAAAAAAMAiFOcAAAAAAAAAi1CcAwAAAAAAACxCcQ4AAAAAAACwCMU5AAAAAMgnU6ZMUZMmTVSqVCm5u7urW7duOnr0qE3M5cuXFRISojJlyqhkyZLq0aOHEhMTbWKOHz+uoKAgFS9eXO7u7ho1apSuXr1qE7N161Y1atRIjo6OqlatmiIiIvL79AAAeYDiHAAAAADkk23btikkJES7du1SdHS0rly5og4dOujSpUtmzMiRI/XFF19o1apV2rZtm06ePKnu3bub7enp6QoKClJaWpp27NihpUuXKiIiQuPHjzdj4uPjFRQUpDZt2iguLk4jRozQoEGDFBUVVaDnCwC4fRTnAAAPNEY0AADy08aNG/X888+rdu3aql+/viIiInT8+HHFxsZKkpKTk7Vo0SJNnz5dbdu2lb+/v5YsWaIdO3Zo165dkqRNmzbp8OHD+vjjj9WgQQN16tRJkydPVnh4uNLS0iRJ8+fPl4+Pj6ZNmyZfX1+FhoaqZ8+emjFjhmXnDgDIHYpzAIAHGiMaAAAFKTk5WZJUunRpSVJsbKyuXLmi9u3bmzG1atVSpUqVtHPnTknSzp07VbduXXl4eJgxgYGBSklJ0aFDh8yY6/eRGZO5j+ykpqYqJSXF5gUAKHiWFue2b9+url27ysvLS3Z2dvrss89s2g3D0Pjx41W+fHk5Ozurffv2+umnn2xizp8/r759+8rFxUVubm4aOHCgLl68aBPzww8/6JFHHpGTk5MqVqyoqVOnZunLqlWrVKtWLTk5Oalu3brasGFDnp8vAKDwYUQDAKCgZGRkaMSIEWrRooXq1KkjSUpISJCDg4Pc3NxsYj08PJSQkGDGXF+Yy2zPbLtZTEpKiv7+++9s+zNlyhS5urqar4oVK971OQIAbp+lxblLly6pfv36Cg8Pz7Z96tSpmj17tubPn6/vvvtOJUqUUGBgoC5fvmzG9O3bV4cOHVJ0dLTWrVun7du3a8iQIWZ7SkqKOnToIG9vb8XGxurdd9/VhAkTtGDBAjNmx44devrppzVw4EDt379f3bp1U7du3XTw4MH8O3kAQKFUWEY0MJoBAO4/ISEhOnjwoJYvX251VyRJY8eOVXJysvk6ceKE1V0CgAdSUSsP3qlTJ3Xq1CnbNsMwNHPmTI0bN05PPPGEJOk///mPPDw89Nlnn6lPnz768ccftXHjRu3Zs0eNGzeWJL3//vvq3Lmz3nvvPXl5eWnZsmVKS0vT4sWL5eDgoNq1aysuLk7Tp083i3izZs1Sx44dNWrUKEnS5MmTFR0drTlz5mj+/PkFcCUAAIWBlSManJ2dbdqmTJmiiRMn5tm5AQCsFRoaag4mqFChgrnd09NTaWlpSkpKssk1iYmJ8vT0NGN2795ts7/MtU+vj7lxPdTExES5uLhkyTGZHB0d5ejoeNfnBgC4O4V2zbn4+HglJCTYjDJwdXVVs2bNbEYquLm5mYU5SWrfvr2KFCmi7777zoxp1aqVHBwczJjAwEAdPXpUf/75pxnD+gwAgMI0ooHRDABwfzAMQ6GhoVqzZo22bNkiHx8fm3Z/f38VK1ZMMTEx5rajR4/q+PHjCggIkCQFBATowIEDOn36tBkTHR0tFxcX+fn5mTHX7yMzJnMfAIDCq9AW5zJHGmQ3yuD6UQju7u427UWLFlXp0qXzZDRDZnt2WJ8BAO4vmSMavvrqqxxHNFzvxhEN2Y1WyGy7WUxOIxocHR3l4uJi8wIA3HtCQkL08ccfKzIyUqVKlVJCQoISEhLMdeBcXV01cOBAhYWF6auvvlJsbKz69++vgIAANW/eXJLUoUMH+fn56bnnntP333+vqKgojRs3TiEhIebIt6FDh+qXX37R6NGjdeTIEc2dO1crV67UyJEjLTt3AEDuFNriXGHHiAYAuD8wogEAkJ/mzZun5ORktW7dWuXLlzdfK1asMGNmzJihLl26qEePHmrVqpU8PT316aefmu329vZat26d7O3tFRAQoGeffVb9+vXTpEmTzBgfHx+tX79e0dHRql+/vqZNm6aFCxcqMDCwQM8XAHD7LF1z7mYyRxokJiaqfPny5vbExEQ1aNDAjLn+RkiSrl69qvPnz+fJaIbM9uywPgMA3B9CQkIUGRmptWvXmiMapGsjGZydnW1GNJQuXVouLi4aPnx4jiMapk6dqoSEhGxHNMyZM0ejR4/WgAEDtGXLFq1cuVLr16+37NwBAPnPMIxbxjg5OSk8PDzHB+VJkre3tzZs2HDT/bRu3Vr79++/7T4CAKxVaEfO+fj4yNPT02aUQUpKir777jubkQpJSUmKjY01Y7Zs2aKMjAw1a9bMjNm+fbuuXLlixkRHR6tmzZp66KGHzBhGMwDAg4kRDQAAAACsZOnIuYsXL+rnn382f46Pj1dcXJxKly6tSpUqacSIEXrjjTdUvXp1+fj46LXXXpOXl5e6desmSfL19VXHjh01ePBgzZ8/X1euXFFoaKj69OkjLy8vSdIzzzyjiRMnauDAgRozZowOHjyoWbNmacaMGeZxX3rpJT366KOaNm2agoKCtHz5cu3du1cLFiwo0OsBACh4jGgAAAAAYCVLi3N79+5VmzZtzJ/DwsIkScHBwYqIiNDo0aN16dIlDRkyRElJSWrZsqU2btwoJycn8z3Lli1TaGio2rVrpyJFiqhHjx6aPXu22e7q6qpNmzYpJCRE/v7+Klu2rMaPH68hQ4aYMQ8//LAiIyM1btw4vfrqq6pevbo+++wz1alTpwCuAgAAAAAAAB5UlhbnWrdufdMRC3Z2dpo0aZLNtKAblS5dWpGRkTc9Tr169fT111/fNKZXr17q1avXzTsMAAAAAAAA5KFCu+YcAAAAAAAAcL+jOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hwAAAAAAABgEYpzAAAAAAAAgEUozgEAAAAAAAAWoTgHAAAAAAAAWITiHAAAAAAAAGARinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hwAAAAAAABgEYpzAAAAAAAAgEUozgEAAAAAAAAWoTgHAAAAAAAAWITiHAAAAAAAAGARinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAD5ZPv27eratau8vLxkZ2enzz77zKbdMAyNHz9e5cuXl7Ozs9q3b6+ffvrJJub8+fPq27evXFxc5ObmpoEDB+rixYs2MT/88IMeeeQROTk5qWLFipo6dWp+nxoAII9QnAMAPNC4aQIA5KdLly6pfv36Cg8Pz7Z96tSpmj17tubPn6/vvvtOJUqUUGBgoC5fvmzG9O3bV4cOHVJ0dLTWrVun7du3a8iQIWZ7SkqKOnToIG9vb8XGxurdd9/VhAkTtGDBgnw/PwDA3SvUxbkJEybIzs7O5lWrVi2z/fLlywoJCVGZMmVUsmRJ9ejRQ4mJiTb7OH78uIKCglS8eHG5u7tr1KhRunr1qk3M1q1b1ahRIzk6OqpatWqKiIgoiNMDABQC3DQBAPJTp06d9MYbb+jJJ5/M0mYYhmbOnKlx48bpiSeeUL169fSf//xHJ0+eNL8s+vHHH7Vx40YtXLhQzZo1U8uWLfX+++9r+fLlOnnypCRp2bJlSktL0+LFi1W7dm316dNHL774oqZPn16QpwoAuEOFujgnSbVr19apU6fM1zfffGO2jRw5Ul988YVWrVqlbdu26eTJk+revbvZnp6erqCgIKWlpWnHjh1aunSpIiIiNH78eDMmPj5eQUFBatOmjeLi4jRixAgNGjRIUVFRBXqeAABrcNMEALBKfHy8EhIS1L59e3Obq6urmjVrpp07d0qSdu7cKTc3NzVu3NiMad++vYoUKaLvvvvOjGnVqpUcHBzMmMDAQB09elR//vlnjsdPTU1VSkqKzQsAUPAKfXGuaNGi8vT0NF9ly5aVJCUnJ2vRokWaPn262rZtK39/fy1ZskQ7duzQrl27JEmbNm3S4cOH9fHHH6tBgwbq1KmTJk+erPDwcKWlpUmS5s+fLx8fH02bNk2+vr4KDQ1Vz549NWPGDMvOGQBQOFh508QNEwDc/xISEiRJHh4eNts9PDzMtoSEBLm7u9u0Fy1aVKVLl7aJyW4f1x8jO1OmTJGrq6v5qlix4t2dEADgjhT64txPP/0kLy8vValSRX379tXx48clSbGxsbpy5YrNDVOtWrVUqVIlmxumunXr2iSqwMBApaSk6NChQ2bM9fvIjMncR064aQKA+5+VN03cMAEA8tvYsWOVnJxsvk6cOGF1lwDggVSoi3PNmjVTRESENm7cqHnz5ik+Pl6PPPKILly4oISEBDk4OMjNzc3mPTfeMN3qZiinmJSUFP3999859o2bJgBAfuKGCQDuf56enpKUZd3sxMREs83T01OnT5+2ab969arOnz9vE5PdPq4/RnYcHR3l4uJi8wIAFLxCXZzr1KmTevXqpXr16ikwMFAbNmxQUlKSVq5caXXXuGkCgAeAlTdN3DABwP3Px8dHnp6eiomJMbelpKTou+++U0BAgCQpICBASUlJio2NNWO2bNmijIwMNWvWzIzZvn27rly5YsZER0erZs2aeuihhwrobAAAd6pQF+du5Obmpho1aujnn3+Wp6en0tLSlJSUZBNz4w3TrW6GcopxcXGRs7Nzjn3hpgkA7n/cNAEA7tbFixcVFxenuLg4SdfWM42Li9Px48dlZ2enESNG6I033tDnn3+uAwcOqF+/fvLy8lK3bt0kSb6+vurYsaMGDx6s3bt369tvv1VoaKj69OkjLy8vSdIzzzwjBwcHDRw4UIcOHdKKFSs0a9YshYWFWXTWAIDbcU8V5y5evKhjx46pfPny8vf3V7FixWxumI4eParjx4/b3DAdOHDAZkRDdHS0XFxc5OfnZ8Zcv4/MmMx9AADub9w0AQDy0969e9WwYUM1bNhQkhQWFqaGDRtq/PjxkqTRo0dr+PDhGjJkiJo0aaKLFy9q48aNcnJyMvexbNky1apVS+3atVPnzp3VsmVLLViwwGx3dXXVpk2bFB8fL39/f7388ssaP368hgwZUrAnCwC4I0Wt7sDN/Otf/1LXrl3l7e2tkydP6vXXX5e9vb2efvppubq6auDAgQoLC1Pp0qXl4uKi4cOHKyAgQM2bN5ckdejQQX5+fnruuec0depUJSQkaNy4cQoJCZGjo6MkaejQoZozZ45Gjx6tAQMGaMuWLVq5cqXWr19v5akDAArI3r171aZNG/PnzIJZcHCwIiIiNHr0aF26dElDhgxRUlKSWrZsme1NU2hoqNq1a6ciRYqoR48emj17ttmeedMUEhIif39/lS1blpsmAHhAtG7dWoZh5NhuZ2enSZMmadKkSTnGlC5dWpGRkTc9Tr169fT111/fcT8BANYp1MW533//XU8//bTOnTuncuXKqWXLltq1a5fKlSsnSZoxY4Z5E5SamqrAwEDNnTvXfL+9vb3WrVunYcOGKSAgQCVKlFBwcLBN4vPx8dH69es1cuRIzZo1SxUqVNDChQsVGBhY4OcLACh43DQBAAAAsFKhLs4tX778pu1OTk4KDw9XeHh4jjHe3t7asGHDTffTunVr7d+//476CAAAAAAAANype2rNOQAAAAAAAOB+QnEOAAAAAAAAsAjFOQAAAAAAAMAiFOcAAAAAAAAAi1CcAwAAAAAAACxCcQ4AAAAAAACwCMU5AAAAAAAAwCIU5wAAAAAAAACLUJwDAAAAAAAALEJxDgAAAAAAALAIxTkAAAAAAADAIhTnAAAAAAAAAItQnAMAAAAAAAAsQnEOAAAAAAAAsAjFOQAAAAAAAMAiFOcAAAAAAAAAi1CcAwAAAAAAACxCcQ4AAAAAAACwCMU5AAAAAAAAwCIU5wAAAAAAAACLUJwDAAAAAAAALEJxDgAAAAAAALAIxTkAAAAAAADAIhTnAAAAAAAAAItQnAMAAAAAAAAsQnEOAAAAAAAAsAjFOQAAAAAAAMAiFOcAAAAAAAAAi1CcAwAAAAAAACxCcQ4AAAAAAACwCMU5AAAAAAAAwCIU524QHh6uypUry8nJSc2aNdPu3but7hIA4D5CngEA5CfyDADceyjOXWfFihUKCwvT66+/rn379ql+/foKDAzU6dOnre4aAOA+QJ4BAOQn8gwA3Jsozl1n+vTpGjx4sPr37y8/Pz/Nnz9fxYsX1+LFi63uGgDgPkCeAQDkJ/IMANybilrdgcIiLS1NsbGxGjt2rLmtSJEiat++vXbu3JklPjU1VampqebPycnJkqSUlJQ77kNG6l93/N77xd1cP4lrmInrmDe4jnnjTq9j5vsMw8jL7liGPFM43O3/a4nrKHEd8wp5Jm+QZ6653Twj5X2u4d/kNfzfzhtcx7tHvs4bd3Mdc5trKM79f2fPnlV6ero8PDxstnt4eOjIkSNZ4qdMmaKJEydm2V6xYsV86+ODwHWm1T24P3Ad8wbXMW/c7XW8cOGCXF1d86QvViLPFA78v84bXMe8wXXMG+SZa243z0jkmvzC/+28wXW8e1zDvJEX1/FWuYbi3B0aO3aswsLCzJ8zMjJ0/vx5lSlTRnZ2dhb27M6kpKSoYsWKOnHihFxcXKzuzj2L65g3uI55416/joZh6MKFC/Ly8rK6K5a43/KMdO//mywsuI53j2uYN+716/ig5xnp/ss19/q/ycKC65g3uI537364hrnNNRTn/r+yZcvK3t5eiYmJNtsTExPl6emZJd7R0VGOjo4229zc3PKziwXCxcXlnv1HX5hwHfMG1zFv3MvX8X4YyZCJPPN/7uV/k4UJ1/HucQ3zxr18HR/kPCPdv7nmXv43WZhwHfMG1/Hu3evXMDe5hgdC/H8ODg7y9/dXTEyMuS0jI0MxMTEKCAiwsGcAgPsBeQYAkJ/IMwBw72Lk3HXCwsIUHBysxo0bq2nTppo5c6YuXbqk/v37W901AMB9gDwDAMhP5BkAuDdRnLtO7969debMGY0fP14JCQlq0KCBNm7cmGVR1fuRo6OjXn/99SzD2nF7uI55g+uYN7iOhc+DnGck/k3mFa7j3eMa5g2uY+FDnuHfZF7gOuYNruPde5CuoZ1xvzw7HAAAAAAAALjHsOYcAAAAAAAAYBGKcwAAAAAAAIBFKM4BAAAAAAAAFqE4BwAAAAAAAFiE4hwkSeHh4apcubKcnJzUrFkz7d692+ou3VO2b9+url27ysvLS3Z2dvrss8+s7tI9acqUKWrSpIlKlSold3d3devWTUePHrW6W/eUefPmqV69enJxcZGLi4sCAgL05ZdfWt0tgDyTB8g1d488kzfINSisyDV3hzxz98gzeeNBzDMU56AVK1YoLCxMr7/+uvbt26f69esrMDBQp0+ftrpr94xLly6pfv36Cg8Pt7or97Rt27YpJCREu3btUnR0tK5cuaIOHTro0qVLVnftnlGhQgW9/fbbio2N1d69e9W2bVs98cQTOnTokNVdwwOMPJM3yDV3jzyTN8g1KIzINXePPHP3yDN540HMM3aGYRhWdwLWatasmZo0aaI5c+ZIkjIyMlSxYkUNHz5cr7zyisW9u/fY2dlpzZo16tatm9VdueedOXNG7u7u2rZtm1q1amV1d+5ZpUuX1rvvvquBAwda3RU8oMgzeY9ckzfIM3mHXAOrkWvyFnkmb5Bn8s79nmcYOfeAS0tLU2xsrNq3b29uK1KkiNq3b6+dO3da2DNASk5OlnTtgxi3Lz09XcuXL9elS5cUEBBgdXfwgCLPoDAjz9w9cg0KA3INCivyzN17UPJMUas7AGudPXtW6enp8vDwsNnu4eGhI0eOWNQr4Nq3nSNGjFCLFi1Up04dq7tzTzlw4IACAgJ0+fJllSxZUmvWrJGfn5/V3cIDijyDwoo8c3fINShMyDUojMgzd+dByzMU5wAUSiEhITp48KC++eYbq7tyz6lZs6bi4uKUnJys1atXKzg4WNu2bbuvkxkA3C7yzN0h1wDAzZFn7s6Dlmcozj3gypYtK3t7eyUmJtpsT0xMlKenp0W9woMuNDRU69at0/bt21WhQgWru3PPcXBwULVq1SRJ/v7+2rNnj2bNmqUPPvjA4p7hQUSeQWFEnrl75BoUJuQaFDbkmbv3oOUZ1px7wDk4OMjf318xMTHmtoyMDMXExNzX87lROBmGodDQUK1Zs0ZbtmyRj4+P1V26L2RkZCg1NdXqbuABRZ5BYUKeyT/kGliJXIPCgjyTf+73PMPIOSgsLEzBwcFq3LixmjZtqpkzZ+rSpUvq37+/1V27Z1y8eFE///yz+XN8fLzi4uJUunRpVapUycKe3VtCQkIUGRmptWvXqlSpUkpISJAkubq6ytnZ2eLe3RvGjh2rTp06qVKlSrpw4YIiIyO1detWRUVFWd01PMDIM3mDXHP3yDN5g1yDwohcc/fIM3ePPJM3Hsg8YwCGYbz//vtGpUqVDAcHB6Np06bGrl27rO7SPeWrr74yJGV5BQcHW921e0p211CSsWTJEqu7ds8YMGCA4e3tbTg4OBjlypUz2rVrZ2zatMnqbgHkmTxArrl75Jm8Qa5BYUWuuTvkmbtHnskbD2KesTMMw8j3CiAAAAAAAACALFhzDgAAAAAAALAIxTkAAAAAAADAIhTnAAAAAAAAAItQnAMAAAAAAAAsQnEOAAAAAAAAsAjFOQAAAAAAAMAiFOcAAAAAAAAAi1CcAwAAAAAAACxCcQ64T9nZ2emzzz6zuhsAgPsUeQYAkJ/IM3iQUJwD7lEJCQkaPny4qlSpIkdHR1WsWFFdu3ZVTEyM1V0DANwHyDMAgPxEngH+T1GrOwDg9v36669q0aKF3Nzc9O6776pu3bq6cuWKoqKiFBISoiNHjljdRQDAPYw8AwDIT+QZwBYj54B70D//+U/Z2dlp9+7d6tGjh2rUqKHatWsrLCxMu3btyvY9Y8aMUY0aNVS8eHFVqVJFr732mq5cuWK2f//992rTpo1KlSolFxcX+fv7a+/evZKk3377TV27dtVDDz2kEiVKqHbt2tqwYUOBnCsAoOCRZwAA+Yk8A9hi5Bxwjzl//rw2btyoN998UyVKlMjS7ubmlu37SpUqpYiICHl5eenAgQMaPHiwSpUqpdGjR0uS+vbtq4YNG2revHmyt7dXXFycihUrJkkKCQlRWlqatm/frhIlSujw4cMqWbJkvp0jAMA65BkAQH4izwBZUZwD7jE///yzDMNQrVq1but948aNM/9cuXJl/etf/9Ly5cvNZHb8+HGNGjXK3G/16tXN+OPHj6tHjx6qW7euJKlKlSp3exoAgEKKPAMAyE/kGSArprUC9xjDMO7ofStWrFCLFi3k6empkiVLaty4cTp+/LjZHhYWpkGDBql9+/Z6++23dezYMbPtxRdf1BtvvKEWLVro9ddf1w8//HDX5wEAKJzIMwCA/ESeAbKiOAfcY6pXry47O7vbWiR1586d6tu3rzp37qx169Zp//79+ve//620tDQzZsKECTp06JCCgoK0ZcsW+fn5ac2aNZKkQYMG6ZdfftFzzz2nAwcOqHHjxnr//ffz/NwAANYjzwAA8hN5BsjKzrjTsjUAy3Tq1EkHDhzQ0aNHs6zTkJSUJDc3N9nZ2WnNmjXq1q2bpk2bprlz59p8ezRo0CCtXr1aSUlJ2R7j6aef1qVLl/T5559naRs7dqzWr1/PN04AcJ8izwAA8hN5BrDFyDngHhQeHq709HQ1bdpUn3zyiX766Sf9+OOPmj17tgICArLEV69eXcePH9fy5ct17NgxzZ492/wWSZL+/vtvhYaGauvWrfrtt9/07bffas+ePfL19ZUkjRgxQlFRUYqPj9e+ffv01VdfmW0AgPsPeQYAkJ/IM4AtHggB3IOqVKmiffv26c0339TLL7+sU6dOqVy5cvL399e8efOyxD/++OMaOXKkQkNDlZqaqqCgIL322muaMGGCJMne3l7nzp1Tv379lJiYqLJly6p79+6aOHGiJCk9PV0hISH6/fff5eLioo4dO2rGjBkFecoAgAJEngEA5CfyDGCLaa0AAAAAAACARZjWCgAAAAAAAFiE4hwAAAAAAABgEYpzAAAAAAAAgEUozgEAAAAAAAAWoTgHAAAAAAAAWITiHAAAAAAAAGARinMAAAAAAACARSjOAQAAAAAAABahOAcAAAAAAABYhOIcAAAAAAAAYBGKcwAAAAAAAIBF/h8gVmjGTVVf+gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1500x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train labels: Counter({np.int64(3): 34878, np.int64(0): 34840, np.int64(1): 34292, np.int64(2): 34254})\n",
            "Dev labels: Counter({np.int64(1): 6855, np.int64(3): 6786, np.int64(2): 6702, np.int64(0): 6633})\n",
            "Test labels: Counter({np.int64(1): 6701, np.int64(2): 6698, np.int64(3): 6568, np.int64(0): 6565})\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_class_distribution(y_train, y_dev, y_test):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    for i, (y, name) in enumerate(zip([y_train, y_dev, y_test], ['Train', 'Dev', 'Test'])):\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        ax[i].bar(unique, counts)\n",
        "        ax[i].set_title(f'{name} set (Total: {len(y)})')\n",
        "        ax[i].set_xlabel('Class')\n",
        "        ax[i].set_ylabel('Count')\n",
        "    plt.show()\n",
        "\n",
        "plot_class_distribution(y_train, y_dev, y_test)\n",
        "from collections import Counter\n",
        "print(\"Train labels:\", Counter(y_train))\n",
        "print(\"Dev labels:\", Counter(y_dev))\n",
        "print(\"Test labels:\", Counter(y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kTQC0fNWXRPv",
      "metadata": {
        "id": "kTQC0fNWXRPv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rpWI0PMuJRJO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "rpWI0PMuJRJO",
        "outputId": "6dd706a4-094e-4ab8-ef80-6bca0618ead1"
      },
      "outputs": [],
      "source": [
        "for xb, yb in train_loader:\n",
        "    print(\"Input shape:\", xb.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W-vynnfRqfov",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "collapsed": true,
        "id": "W-vynnfRqfov",
        "outputId": "5fc8f12f-8e27-4fbf-f70d-ef940162532c"
      },
      "outputs": [],
      "source": [
        "import os, time, json, pprint\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from braindecode.models import EEGNetv4\n",
        "from braindecode.util import set_random_seeds\n",
        "\n",
        "def ensure_nct(x: np.ndarray, n_chans: int) -> np.ndarray:\n",
        "    if x.ndim != 3:\n",
        "        raise ValueError(f\"Expected 3D array, got {x.shape}\")\n",
        "    if x.shape[1] == n_chans:\n",
        "        return x.astype(np.float32, copy=False)\n",
        "    if x.shape[2] == n_chans:\n",
        "        return x.transpose(0, 2, 1).astype(np.float32, copy=False)\n",
        "    raise ValueError(f\"Cannot infer channels dim for shape {x.shape} with n_chans={n_chans}\")\n",
        "\n",
        "N_CHANS = 64\n",
        "X_train = ensure_nct(X_train, N_CHANS)\n",
        "X_dev   = ensure_nct(X_dev,   N_CHANS)\n",
        "X_test  = ensure_nct(X_test,  N_CHANS)\n",
        "\n",
        "y_train = y_train.astype(np.int64, copy=False)\n",
        "y_dev   = y_dev.astype(np.int64,   copy=False)\n",
        "y_test  = y_test.astype(np.int64,  copy=False)\n",
        "\n",
        "n_classes = int(np.unique(y_train).size)\n",
        "input_window_samples = X_train.shape[2]\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def make_loaders(batch_size: int):\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n",
        "        batch_size=batch_size, shuffle=True, pin_memory=(device.type == 'cuda')\n",
        "    )\n",
        "    dev_loader = DataLoader(\n",
        "        TensorDataset(torch.from_numpy(X_dev), torch.from_numpy(y_dev)),\n",
        "        batch_size=batch_size, shuffle=False, pin_memory=(device.type == 'cuda')\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test)),\n",
        "        batch_size=batch_size, shuffle=False, pin_memory=(device.type == 'cuda')\n",
        "    )\n",
        "    return train_loader, dev_loader, test_loader\n",
        "\n",
        "def make_model(drop_prob: float):\n",
        "    set_random_seeds(seed=2023, cuda=(device.type == 'cuda'))\n",
        "    model = EEGNetv4(\n",
        "        n_chans=N_CHANS,\n",
        "        n_outputs=n_classes,\n",
        "        input_window_samples=input_window_samples,\n",
        "        pool_mode='mean',\n",
        "        drop_prob=drop_prob,\n",
        "        final_conv_length='auto'\n",
        "    ).to(device)\n",
        "    return model\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        preds = out.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.numel()\n",
        "    return total_loss / max(total, 1), correct / max(total, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_loader(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        out = model(xb)\n",
        "        preds = out.argmax(1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(yb.numpy())\n",
        "    metrics = {\n",
        "        \"accuracy\": float(accuracy_score(all_labels, all_preds)),\n",
        "        \"precision_w\": float(precision_score(all_labels, all_preds, average='weighted', zero_division=0)),\n",
        "        \"recall_w\": float(recall_score(all_labels, all_preds, average='weighted', zero_division=0)),\n",
        "        \"f1_w\": float(f1_score(all_labels, all_preds, average='weighted', zero_division=0)),\n",
        "        \"confusion_matrix\": confusion_matrix(all_labels, all_preds).tolist(),\n",
        "        \"classification_report\": classification_report(all_labels, all_preds, zero_division=0)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "experiment_list = [\n",
        "    {\"trial\": 1, \"learning_rate\": 1e-3,  \"batch_size\": 64,  \"epochs\": 30, \"dropout\": 0.5},\n",
        "    {\"trial\": 2, \"learning_rate\": 5e-4,  \"batch_size\": 32,  \"epochs\": 50, \"dropout\": 0.5},\n",
        "    {\"trial\": 3, \"learning_rate\": 1e-4,  \"batch_size\": 64,  \"epochs\": 75, \"dropout\": 0.4},\n",
        "    {\"trial\": 4, \"learning_rate\": 2e-4,  \"batch_size\": 128, \"epochs\": 50, \"dropout\": 0.5},\n",
        "]\n",
        "\n",
        "ALL_RESULTS = []\n",
        "for exp in experiment_list:\n",
        "    trial   = exp[\"trial\"]\n",
        "    lr      = exp[\"learning_rate\"]\n",
        "    bs      = exp[\"batch_size\"]\n",
        "    epochs  = exp[\"epochs\"]\n",
        "    dropout = exp.get(\"dropout\", 0.5)\n",
        "\n",
        "    print(f\"\\nRunning Trial {trial} — LR={lr}, Batch Size={bs}, Epochs={epochs}, Dropout={dropout}\")\n",
        "\n",
        "    train_loader, dev_loader, test_loader = make_loaders(bs)\n",
        "\n",
        "    model = make_model(drop_prob=dropout)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start_train = time.perf_counter()\n",
        "    best_dev_f1 = -1.0\n",
        "    best_ckpt_path = f\"eegnetv4_trial{trial}.pt\"\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "        dev_metrics = evaluate_loader(model, dev_loader)\n",
        "        print(f\"Epoch {epoch:>3}/{epochs} — train_loss: {tr_loss:.4f} | train_acc: {tr_acc:.4f} | dev_f1_w: {dev_metrics['f1_w']:.4f} | dev_acc: {dev_metrics['accuracy']:.4f}\")\n",
        "        if dev_metrics['f1_w'] > best_dev_f1:\n",
        "            best_dev_f1 = dev_metrics['f1_w']\n",
        "            torch.save(model.state_dict(), best_ckpt_path)\n",
        "\n",
        "    train_time = time.perf_counter() - start_train\n",
        "    print(f\"Training time: {train_time:.2f}s (trial {trial})\")\n",
        "\n",
        "    model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "    train_metrics = evaluate_loader(model, train_loader)\n",
        "    dev_metrics   = evaluate_loader(model, dev_loader)\n",
        "    test_metrics  = evaluate_loader(model, test_loader)\n",
        "\n",
        "    results = {\n",
        "        \"trial\": trial,\n",
        "        \"learning_rate\": lr,\n",
        "        \"batch_size\": bs,\n",
        "        \"epochs\": epochs,\n",
        "        \"dropout\": dropout,\n",
        "        \"train_time_seconds\": train_time,\n",
        "        \"train_metrics\": train_metrics,\n",
        "        \"dev_metrics\": dev_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"checkpoint_path\": best_ckpt_path\n",
        "    }\n",
        "    ALL_RESULTS.append(results)\n",
        "    pprint.pprint(results)\n",
        "\n",
        "    with open(f\"eegnetv4_across_subjects_trial{trial}.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    with open(f\"eegnetv4_across_subjects_trial{trial}.txt\", \"w\") as f:\n",
        "        f.write(json.dumps(results, indent=4))\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(f\"eegnetv4_across_subjects_trial{trial}.json\")\n",
        "        files.download(f\"eegnetv4_across_subjects_trial{trial}.txt\")\n",
        "        files.download(best_ckpt_path)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "with open(\"eegnetv4_all_trials_summary.json\", \"w\") as f:\n",
        "    json.dump(ALL_RESULTS, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTTiR8ort9wU",
      "metadata": {
        "id": "UTTiR8ort9wU"
      },
      "source": [
        "new additiob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WO6U1C1rt_Mw",
      "metadata": {
        "id": "WO6U1C1rt_Mw"
      },
      "outputs": [],
      "source": [
        "# # Dataloaders\n",
        "# batch_size = 64\n",
        "# train_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=batch_size, shuffle=True)\n",
        "# dev_loader = DataLoader(TensorDataset(torch.tensor(X_dev), torch.tensor(y_dev)), batch_size=batch_size)\n",
        "# test_loader = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suLvWQVwA92g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "suLvWQVwA92g",
        "outputId": "87d60fe7-2f59-41d5-d09b-b8f990e0a63c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔸 Running Trial 3 — LR=0.0001, Batch Size=32, Epochs=30\n",
            "Epoch 1/30 — Loss: 6104.0329\n",
            "Epoch 2/30 — Loss: 5924.1421\n",
            "Epoch 3/30 — Loss: 5912.3782\n",
            "Epoch 4/30 — Loss: 5791.8336\n",
            "Epoch 5/30 — Loss: 5714.1789\n",
            "Epoch 6/30 — Loss: 5633.3907\n",
            "Epoch 7/30 — Loss: 5568.4023\n",
            "Epoch 8/30 — Loss: 5483.9981\n",
            "Epoch 9/30 — Loss: 5441.8832\n",
            "Epoch 10/30 — Loss: 5361.1386\n",
            "Epoch 11/30 — Loss: 5256.1566\n",
            "Epoch 12/30 — Loss: 5136.0357\n",
            "Epoch 13/30 — Loss: 5006.2822\n",
            "Epoch 14/30 — Loss: 4856.9524\n",
            "Epoch 15/30 — Loss: 4688.4227\n",
            "Epoch 16/30 — Loss: 4493.6697\n",
            "Epoch 17/30 — Loss: 4271.6848\n",
            "Epoch 18/30 — Loss: 4031.4726\n",
            "Epoch 19/30 — Loss: 3754.0275\n",
            "Epoch 20/30 — Loss: 3453.7116\n",
            "Epoch 21/30 — Loss: 3149.8573\n",
            "Epoch 22/30 — Loss: 2862.7199\n",
            "Epoch 23/30 — Loss: 2579.9359\n",
            "Epoch 24/30 — Loss: 2324.4226\n",
            "Epoch 25/30 — Loss: 2116.7035\n",
            "Epoch 26/30 — Loss: 1903.4852\n",
            "Epoch 27/30 — Loss: 1715.3092\n",
            "Epoch 28/30 — Loss: 1566.2442\n",
            "Epoch 29/30 — Loss: 1432.6194\n",
            "Epoch 30/30 — Loss: 1305.6715\n",
            "Saved metrics and model for Trial 3\n",
            "🔹 Evaluating on train set...\n",
            "🔹 Evaluating on Dev set...\n",
            "🔹 Evaluating on Test set...\n",
            "{\n",
            "    \"trial\": 3,\n",
            "    \"learning_rate\": 0.0001,\n",
            "    \"batch_size\": 32,\n",
            "    \"epochs\": 30,\n",
            "    \"train_time_seconds\": 35029.199974287,\n",
            "    \"train_metrics\": {\n",
            "        \"accuracy\": 0.9061577851067523,\n",
            "        \"precision_weighted\": 0.9080854340500734,\n",
            "        \"recall_weighted\": 0.9061577851067523,\n",
            "        \"f1_weighted\": 0.9061523582375935,\n",
            "        \"precision_macro\": 0.9080619378486223,\n",
            "        \"recall_macro\": 0.9062188257556718,\n",
            "        \"f1_macro\": 0.9061683425925509,\n",
            "        \"matthews_corrcoef\": 0.8755035969837002,\n",
            "        \"cohen_kappa\": 0.8748783161862072,\n",
            "        \"auc\": 0.990497449622387,\n",
            "        \"confusion_matrix\": [\n",
            "            [\n",
            "                32437,\n",
            "                780,\n",
            "                923,\n",
            "                700\n",
            "            ],\n",
            "            [\n",
            "                2158,\n",
            "                30233,\n",
            "                1100,\n",
            "                801\n",
            "            ],\n",
            "            [\n",
            "                689,\n",
            "                409,\n",
            "                32244,\n",
            "                912\n",
            "            ],\n",
            "            [\n",
            "                1096,\n",
            "                539,\n",
            "                2868,\n",
            "                30375\n",
            "            ]\n",
            "        ],\n",
            "        \"classification_report\": \"              precision    recall  f1-score   support\\n\\n           0     0.8916    0.9310    0.9109     34840\\n           1     0.9459    0.8816    0.9127     34292\\n           2     0.8683    0.9413    0.9033     34254\\n           3     0.9264    0.8709    0.8978     34878\\n\\n    accuracy                         0.9062    138264\\n   macro avg     0.9081    0.9062    0.9062    138264\\nweighted avg     0.9081    0.9062    0.9062    138264\\n\",\n",
            "        \"time_seconds\": 367.63007158500113\n",
            "    },\n",
            "    \"dev_metrics\": {\n",
            "        \"accuracy\": 0.2930753262158956,\n",
            "        \"precision_weighted\": 0.29311957514558984,\n",
            "        \"recall_weighted\": 0.2930753262158956,\n",
            "        \"f1_weighted\": 0.2880974916408006,\n",
            "        \"precision_macro\": 0.29309513472804194,\n",
            "        \"recall_macro\": 0.29370457946642214,\n",
            "        \"f1_macro\": 0.2883785688366497,\n",
            "        \"matthews_corrcoef\": 0.05871185561129621,\n",
            "        \"cohen_kappa\": 0.05806376524515644,\n",
            "        \"auc\": 0.5443000484886119,\n",
            "        \"confusion_matrix\": [\n",
            "            [\n",
            "                2747,\n",
            "                1520,\n",
            "                1469,\n",
            "                897\n",
            "            ],\n",
            "            [\n",
            "                2103,\n",
            "                1989,\n",
            "                1666,\n",
            "                1097\n",
            "            ],\n",
            "            [\n",
            "                2267,\n",
            "                1484,\n",
            "                1833,\n",
            "                1118\n",
            "            ],\n",
            "            [\n",
            "                2068,\n",
            "                1726,\n",
            "                1655,\n",
            "                1337\n",
            "            ]\n",
            "        ],\n",
            "        \"classification_report\": \"              precision    recall  f1-score   support\\n\\n           0     0.2991    0.4141    0.3473      6633\\n           1     0.2960    0.2902    0.2931      6855\\n           2     0.2768    0.2735    0.2751      6702\\n           3     0.3005    0.1970    0.2380      6786\\n\\n    accuracy                         0.2931     26976\\n   macro avg     0.2931    0.2937    0.2884     26976\\nweighted avg     0.2931    0.2931    0.2881     26976\\n\",\n",
            "        \"time_seconds\": 71.84175383299589\n",
            "    },\n",
            "    \"test_metrics\": {\n",
            "        \"accuracy\": 0.29093170511080957,\n",
            "        \"precision_weighted\": 0.2938123673780835,\n",
            "        \"recall_weighted\": 0.29093170511080957,\n",
            "        \"f1_weighted\": 0.28869694627567977,\n",
            "        \"precision_macro\": 0.29381836467951683,\n",
            "        \"recall_macro\": 0.29084234435931106,\n",
            "        \"f1_macro\": 0.2886296488521911,\n",
            "        \"matthews_corrcoef\": 0.054869155506621775,\n",
            "        \"cohen_kappa\": 0.054439752893404636,\n",
            "        \"auc\": 0.5461490880576928,\n",
            "        \"confusion_matrix\": [\n",
            "            [\n",
            "                2319,\n",
            "                1338,\n",
            "                1905,\n",
            "                1003\n",
            "            ],\n",
            "            [\n",
            "                1879,\n",
            "                1940,\n",
            "                1865,\n",
            "                1017\n",
            "            ],\n",
            "            [\n",
            "                2092,\n",
            "                1405,\n",
            "                2089,\n",
            "                1112\n",
            "            ],\n",
            "            [\n",
            "                1843,\n",
            "                1435,\n",
            "                1919,\n",
            "                1371\n",
            "            ]\n",
            "        ],\n",
            "        \"classification_report\": \"              precision    recall  f1-score   support\\n\\n           0     0.2851    0.3532    0.3156      6565\\n           1     0.3171    0.2895    0.3027      6701\\n           2     0.2686    0.3119    0.2886      6698\\n           3     0.3045    0.2087    0.2477      6568\\n\\n    accuracy                         0.2909     26532\\n   macro avg     0.2938    0.2908    0.2886     26532\\nweighted avg     0.2938    0.2909    0.2887     26532\\n\",\n",
            "        \"time_seconds\": 71.00421126299625\n",
            "    }\n",
            "}\n",
            "✅ Saved metrics and model for Trial 3\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2059614825.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'perceiver_across_subjects_trial{trial}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Saved metrics and model for Trial {trial}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'perceiver_across_subjects_trial{trial}.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'perceiver_across_subjects_trial{trial}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "experiment_list_Perceiver = [\n",
        "      # {'trial': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 15,\n",
        "      #   'd_latents': 256, 'd_model': 256, 'num_latents': 128, 'num_blocks': 4},\n",
        "\n",
        "      {\n",
        "\n",
        "      'trial': 3,\n",
        "      \"learning_rate\": 0.0001,\n",
        "      \"batch_size\": 32,\n",
        "      \"epochs\": 30,\n",
        "      \"d_latents\": 256,\n",
        "      \"d_model\": 256,\n",
        "      \"num_latents\": 128,\n",
        "      \"num_blocks\": 4\n",
        "\n",
        "      },\n",
        "\n",
        "\n",
        "    {'trial': 2, 'learning_rate': 5e-4, 'batch_size': 32, 'epochs': 20,\n",
        "     'd_latents': 256, 'd_model': 256, 'num_latents': 128, 'num_blocks': 5},\n",
        "\n",
        "    {'trial': 3, 'learning_rate': 1e-4, 'batch_size': 64, 'epochs': 50,\n",
        "     'd_latents': 512, 'd_model': 512, 'num_latents': 128, 'num_blocks': 4},\n",
        "\n",
        "    {'trial': 4, 'learning_rate': 5e-4, 'batch_size': 64, 'epochs': 100,\n",
        "     'd_latents': 512, 'd_model': 512, 'num_latents': 256, 'num_blocks': 6},\n",
        "\n",
        "    {'trial': 5, 'learning_rate': 1e-3, 'batch_size': 128, 'epochs': 100,\n",
        "     'd_latents': 512, 'd_model': 512, 'num_latents': 256, 'num_blocks': 8}\n",
        "]\n",
        "\n",
        "\n",
        "for exp in experiment_list_Perceiver:\n",
        "    trial, lr, batch_size, epochs = exp['trial'], exp['learning_rate'], exp['batch_size'], exp['epochs']\n",
        "\n",
        "    print(f\"\\nRunning Trial {trial} — LR={lr}, Batch Size={batch_size}, Epochs={epochs}\")\n",
        "\n",
        "\n",
        "    class PerceiverClassifier(nn.Module):\n",
        "        def __init__(self, num_classes):\n",
        "            super().__init__()\n",
        "            config = PerceiverConfig(\n",
        "    input_dim=256,\n",
        "    d_latents=exp['d_latents'],\n",
        "    d_model=exp['d_model'],\n",
        "    num_latents=exp['num_latents'],\n",
        "    num_blocks=exp['num_blocks']\n",
        ")\n",
        "            self.input_proj = nn.Linear(no_feature, 256)\n",
        "            self.perceiver = PerceiverModel(config)\n",
        "            self.classifier = nn.Linear(config.d_model, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.input_proj(x)\n",
        "            out = self.perceiver(inputs=x).last_hidden_state\n",
        "            return self.classifier(out.mean(dim=1))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = PerceiverClassifier(num_classes=n_class).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Timing training\n",
        "    train_start = time.perf_counter()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} — Loss: {total_loss:.4f}\")\n",
        "    train_end = time.perf_counter()\n",
        "    train_time = train_end - train_start\n",
        "\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), f'lPer_across_subjects_trial{trial}.pt')\n",
        "    print(f\"Saved metrics and model for Trial {trial}\")\n",
        "\n",
        "    # Evaluate with time tracking\n",
        "    def timed_evaluate(loader):\n",
        "        start_eval = time.perf_counter()\n",
        "        preds, labels, probs = [], [], []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in loader:\n",
        "                out = model(xb.to(device))\n",
        "                soft_out = F.softmax(out, dim=1).cpu().numpy()\n",
        "                probs.extend(soft_out)\n",
        "                preds.extend(np.argmax(soft_out, axis=1))\n",
        "                if yb.device.type == 'cuda':\n",
        "                    labels.extend(yb.cpu().numpy())\n",
        "                else:\n",
        "                    labels.extend(yb.numpy())\n",
        "\n",
        "        end_eval = time.perf_counter()\n",
        "        encoder = OneHotEncoder(sparse_output=False)\n",
        "        y_true_onehot = encoder.fit_transform(np.array(labels).reshape(-1, 1))\n",
        "        auc = roc_auc_score(y_true_onehot, np.array(probs))\n",
        "        report = classification_report(labels, preds, digits=4, output_dict=False)\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(labels, preds),\n",
        "            'precision_weighted': precision_score(labels, preds, average='weighted'),\n",
        "            'recall_weighted': recall_score(labels, preds, average='weighted'),\n",
        "            'f1_weighted': f1_score(labels, preds, average='weighted'),\n",
        "            'precision_macro': precision_score(labels, preds, average='macro'),\n",
        "            'recall_macro': recall_score(labels, preds, average='macro'),\n",
        "            'f1_macro': f1_score(labels, preds, average='macro'),\n",
        "            'matthews_corrcoef': matthews_corrcoef(labels, preds),\n",
        "            'cohen_kappa': cohen_kappa_score(labels, preds),\n",
        "            'auc': auc,\n",
        "            'confusion_matrix': confusion_matrix(labels, preds).tolist(),\n",
        "            'classification_report': report,\n",
        "            'time_seconds': end_eval - start_eval\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    print(\"Evaluating on train set...\")\n",
        "    train_metrics = timed_evaluate(train_loader)\n",
        "    print(\"Evaluating on Dev set...\")\n",
        "    dev_metrics = timed_evaluate(dev_loader)\n",
        "    print(\"Evaluating on Test set...\")\n",
        "    test_metrics = timed_evaluate(test_loader)\n",
        "\n",
        "    all_results = {\n",
        "        'trial': trial,\n",
        "        'learning_rate': lr,\n",
        "        'batch_size': batch_size,\n",
        "        'epochs': epochs,\n",
        "        'train_time_seconds': train_time,\n",
        "        'train_metrics': train_metrics,\n",
        "        'dev_metrics': dev_metrics,\n",
        "        'test_metrics': test_metrics\n",
        "    }\n",
        "\n",
        "    print(json.dumps(all_results, indent=4))\n",
        "\n",
        "\n",
        "    # Save metrics\n",
        "    with open(f'perceiver_across_subjects_trial{trial}.json', 'w') as f:\n",
        "        json.dump(all_results, f, indent=4)\n",
        "    with open(f'perceiver_across_subjects_trial{trial}.txt', 'w') as f:\n",
        "        f.write(json.dumps(all_results, indent=4))\n",
        "\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f'perceiver_across_subjects_trial{trial}.pt')\n",
        "    print(f\" Saved metrics and model for Trial {trial}\")\n",
        "    files.download(f'perceiver_across_subjects_trial{trial}.json')\n",
        "    files.download(f'perceiver_across_subjects_trial{trial}.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "011c1e91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "011c1e91",
        "outputId": "f1157409-b2d4-4d2e-e105-fbfaab9da169"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kafSF_oDsRNz",
      "metadata": {
        "id": "kafSF_oDsRNz"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BqFLIyp8gBRL",
      "metadata": {
        "id": "BqFLIyp8gBRL"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
