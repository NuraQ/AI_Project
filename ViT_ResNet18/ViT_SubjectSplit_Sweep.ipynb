{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fd99aa",
   "metadata": {},
   "source": [
    "# ViT (5-channel) â€” Experiment Sweep with Subject-ID Loader\n",
    "\n",
    "This notebook preserves the original ViT experiment sweep and **replaces the data loading** with a subject-based loader\n",
    "that reads per-sample `.npz` files from Google Drive, performs segmentation, aggregates segments to single images,\n",
    "and prepares `(N, 5, 8, 8)` tensors for ViT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers==4.42.4 timm==0.9.16 torchvision --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, random, csv\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTConfig, ViTForImageClassification, TrainingArguments, Trainer, set_seed\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "DATA_DIR = '/content/gdrive/MyDrive/preprocessed_per_sample'\n",
    "RESULTS_ROOT = './vit_experiments_subject_split'\n",
    "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "IMAGE_SIZE = 8\n",
    "PATCH_SIZE = 2\n",
    "NUM_CHANNELS = 5\n",
    "\n",
    "EXCLUDED_SUBJECTS = {\"S038\", \"S088\", \"S089\", \"S092\", \"S100\", \"S104\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_ids():\n",
    "    ids = set()\n",
    "    for fname in os.listdir(DATA_DIR):\n",
    "        if fname.endswith('.npz') and fname.startswith('sample'):\n",
    "            sid = fname.split('_')[1][:4]\n",
    "            if sid not in EXCLUDED_SUBJECTS:\n",
    "                ids.add(sid)\n",
    "    return sorted(ids)\n",
    "\n",
    "subject_ids = get_subject_ids()\n",
    "random.shuffle(subject_ids)\n",
    "\n",
    "n_total = len(subject_ids)\n",
    "n_train = min(70, n_total)\n",
    "n_dev = min(15, max(0, n_total - n_train))\n",
    "n_test = min(16, max(0, n_total - n_train - n_dev))\n",
    "\n",
    "train_ids = subject_ids[:n_train]\n",
    "dev_ids   = subject_ids[n_train:n_train+n_dev]\n",
    "test_ids  = subject_ids[n_train+n_dev:n_train+n_dev+n_test]\n",
    "\n",
    "print('Subjects:', len(subject_ids))\n",
    "print('Train IDs:', len(train_ids))\n",
    "print('Dev IDs  :', len(dev_ids))\n",
    "print('Test IDs :', len(test_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_topomaps(X, y, window=16, stride=10):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    num_frames = X.shape[0]\n",
    "    for start in range(0, num_frames - window + 1, stride):\n",
    "        end = start + window\n",
    "        segments.append(X[start:end])\n",
    "        labels.append(y)\n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "def aggregate_segment(seg, method='mean'):\n",
    "    if method == 'mean':\n",
    "        return seg.mean(axis=0)\n",
    "    elif method == 'median':\n",
    "        return np.median(seg, axis=0)\n",
    "    else:\n",
    "        return seg[seg.shape[0]//2]\n",
    "\n",
    "def load_subjects(subject_ids, window=16, stride=10, aggregate='mean', target_channels=NUM_CHANNELS, target_size=IMAGE_SIZE):\n",
    "    X_all, y_all = [], []\n",
    "    for fname in os.listdir(DATA_DIR):\n",
    "        if not (fname.endswith('.npz') and fname.startswith('sample')):\n",
    "            continue\n",
    "        for sid in subject_ids:\n",
    "            if f'_{sid}R' in fname:\n",
    "                data = np.load(os.path.join(DATA_DIR, fname), allow_pickle=True)\n",
    "                X = data['X']\n",
    "                y = int(data['y'])\n",
    "                segs, labels = segment_topomaps(X, y, window=window, stride=stride)\n",
    "                for seg in segs:\n",
    "                    img = aggregate_segment(seg, method=aggregate)\n",
    "                    if img.ndim == 3:\n",
    "                        pass\n",
    "                    elif img.ndim == 4 and img.shape[0] == 1:\n",
    "                        img = img[0]\n",
    "                    else:\n",
    "                        raise ValueError(f'Unexpected segment shape: {img.shape}')\n",
    "                    c_in, h, w = img.shape\n",
    "                    t = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
    "                    t = F.interpolate(t, size=(target_size, target_size), mode='area')\n",
    "                    t = t.squeeze(0)\n",
    "                    if c_in == target_channels:\n",
    "                        t_out = t\n",
    "                    elif c_in < target_channels:\n",
    "                        reps = [t] + [t[-1:].clone() for _ in range(target_channels - c_in)]\n",
    "                        t_out = torch.cat(reps, dim=0)\n",
    "                    else:\n",
    "                        t_out = t[:target_channels]\n",
    "                    X_all.append(t_out.numpy())\n",
    "                    y_all.append(y)\n",
    "    X_all = np.stack(X_all, axis=0) if len(X_all) else np.empty((0, target_channels, target_size, target_size), dtype=np.float32)\n",
    "    y_all = np.array(y_all, dtype=np.int64)\n",
    "    return X_all, y_all\n",
    "\n",
    "def filter_and_map_labels(y, keep=(2,3,6), mapping={2:0, 3:1, 6:2}):\n",
    "    mask = np.isin(y, list(keep))\n",
    "    y_f = y[mask]\n",
    "    y_m = np.vectorize(mapping.get)(y_f)\n",
    "    return mask, y_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_subjects(train_ids, window=16, stride=10, aggregate='mean')\n",
    "X_dev,   y_dev   = load_subjects(dev_ids,   window=16, stride=10, aggregate='mean')\n",
    "X_test,  y_test  = load_subjects(test_ids,  window=16, stride=10, aggregate='mean')\n",
    "\n",
    "m_tr, y_train_m = filter_and_map_labels(y_train)\n",
    "m_de, y_dev_m   = filter_and_map_labels(y_dev)\n",
    "m_te, y_test_m  = filter_and_map_labels(y_test)\n",
    "\n",
    "X_train = X_train[m_tr]\n",
    "X_dev   = X_dev[m_de]\n",
    "X_test  = X_test[m_te]\n",
    "\n",
    "print('Train:', X_train.shape, y_train_m.shape)\n",
    "print('Dev  :', X_dev.shape,   y_dev_m.shape)\n",
    "print('Test :', X_test.shape,  y_test_m.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"pixel_values\": self.X[idx], \"labels\": self.y[idx]}\n",
    "\n",
    "train_ds = EEGDataset(X_train, y_train_m)\n",
    "val_ds   = EEGDataset(X_dev,   y_dev_m)\n",
    "test_ds  = EEGDataset(X_test,  y_test_m)\n",
    "\n",
    "class_weights = torch.tensor(\n",
    "    compute_class_weight('balanced', classes=np.unique(y_train_m), y=y_train_m),\n",
    "    dtype=torch.float\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80369450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_full_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(p.predictions), dim=-1).numpy()\n",
    "    try:\n",
    "        auc = roc_auc_score(p.label_ids, probs, multi_class='ovr', average='weighted')\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"auc\": auc,\n",
    "        \"precision_weighted\": precision_score(p.label_ids, preds, average=\"weighted\", zero_division=0),\n",
    "        \"recall_weighted\": recall_score(p.label_ids, preds, average=\"weighted\", zero_division=0),\n",
    "        \"f1_weighted\": f1_score(p.label_ids, preds, average=\"weighted\", zero_division=0),\n",
    "        \"precision_macro\": precision_score(p.label_ids, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(p.label_ids, preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(p.label_ids, preds, average=\"macro\", zero_division=0),\n",
    "        \"matthews_corrcoef\": matthews_corrcoef(p.label_ids, preds),\n",
    "        \"cohen_kappa\": cohen_kappa_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ccb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(cfg, run_idx):\n",
    "    seed = cfg.get(\"seed\", 42)\n",
    "    set_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    cfg_tag = f\"LR{cfg.get('lr',1e-5)}_E{cfg.get('epochs',60)}_HS{cfg.get('hidden_size',64)}_L{cfg.get('num_hidden_layers',2)}_H{cfg.get('num_attention_heads',4)}_IS{cfg.get('intermediate_size',128)}_WD{cfg.get('weight_decay',0.01)}_DO{cfg.get('dropout',0.1)}_S{seed}\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = Path(RESULTS_ROOT) / f\"run{run_idx:02d}_{ts}_{cfg_tag}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(run_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "    vit_cfg = ViTConfig(\n",
    "        image_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_channels=NUM_CHANNELS,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        hidden_size=cfg.get(\"hidden_size\", 64),\n",
    "        num_hidden_layers=cfg.get(\"num_hidden_layers\", 2),\n",
    "        num_attention_heads=cfg.get(\"num_attention_heads\", 4),\n",
    "        intermediate_size=cfg.get(\"intermediate_size\", 128),\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=cfg.get(\"dropout\", 0.1),\n",
    "        attention_probs_dropout_prob=cfg.get(\"dropout\", 0.1),\n",
    "    )\n",
    "\n",
    "    model = ViTForImageClassification(vit_cfg)\n",
    "\n",
    "    fp16_flag = cfg.get(\"fp16\", torch.cuda.is_available())\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(run_dir / \"hf_out\"),\n",
    "        per_device_train_batch_size=cfg.get(\"batch_size\", 32),\n",
    "        per_device_eval_batch_size=cfg.get(\"batch_size\", 32),\n",
    "        num_train_epochs=cfg.get(\"epochs\", 60),\n",
    "        learning_rate=cfg.get(\"lr\", 1e-5),\n",
    "        weight_decay=cfg.get(\"weight_decay\", 0.01),\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1_weighted\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "        fp16=fp16_flag\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=trainer.train_dataset if False else train_ds,\n",
    "        eval_dataset=trainer.eval_dataset if False else val_ds,\n",
    "        compute_metrics=compute_full_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"===== RUN {run_idx} =====\")\n",
    "    print(f\"Config: {cfg}\")\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    with open(run_dir / \"train_state.json\", \"w\") as f:\n",
    "        json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "    def eval_split(dset, name):\n",
    "        preds = trainer.predict(dset)\n",
    "        metrics = compute_full_metrics(preds)\n",
    "        cm = confusion_matrix(preds.label_ids, np.argmax(preds.predictions, axis=1))\n",
    "        rep = classification_report(preds.label_ids, np.argmax(preds.predictions, axis=1), digits=4, zero_division=0)\n",
    "\n",
    "        split_dir = run_dir / f\"{name}\"\n",
    "        split_dir.mkdir(exist_ok=True, parents=True)\n",
    "        with open(split_dir / \"metrics.json\", \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        np.save(split_dir / \"confusion_matrix.npy\", cm)\n",
    "        np.savetxt(split_dir / \"confusion_matrix.csv\", cm, fmt=\"%d\", delimiter=\",\")\n",
    "        with open(split_dir / \"classification_report.txt\", \"w\") as f:\n",
    "            f.write(rep)\n",
    "        np.save(split_dir / \"predictions.npy\", preds.predictions)\n",
    "        np.save(split_dir / \"labels.npy\", preds.label_ids)\n",
    "\n",
    "        print(f\"â€” {name.upper()} METRICS â€”\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k:20}: {v:.6f}\" if isinstance(v, (int, float, np.floating)) else f\"{k:20}: {v}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"Classification Report:\")\n",
    "        print(rep)\n",
    "\n",
    "        summary_keys = [\"accuracy\",\"auc\",\"f1_weighted\",\"f1_macro\",\"precision_macro\",\"recall_macro\",\"matthews_corrcoef\",\"cohen_kappa\"]\n",
    "        return {k: float(metrics[k]) for k in summary_keys}\n",
    "\n",
    "    summary = {\"config\": cfg}\n",
    "    summary[\"train\"] = eval_split(train_ds, \"train\")\n",
    "    summary[\"val\"]   = eval_split(val_ds,   \"val\")\n",
    "    summary[\"test\"]  = eval_split(test_ds,  \"test\")\n",
    "\n",
    "    with open(run_dir / \"summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    trainer.save_model(str(run_dir / \"best_model\"))\n",
    "\n",
    "    return summary, run_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10729065",
   "metadata": {},
   "outputs": [],
   "source": [
    "SWEEP = [\n",
    "    {\n",
    "        \"run_name\": \"S1_reg_ema_es\",\n",
    "        \"lr\": 2e-5, \"weight_decay\": 0.02, \"batch_size\": 32, \"epochs\": 100,\n",
    "        \"augment\": True, \"weighted_sampler\": True, \"freeze_backbone\": False,\n",
    "        \"label_smoothing\": 0.10, \"lr_schedule\": \"cosine_with_restarts\",\n",
    "        \"num_cycles\": 5, \"warmup_ratio\": 0.15, \"ema_decay\": 0.999,\n",
    "        \"early_stopping\": {\"metric\": \"macro_f1\", \"patience\": 10}\n",
    "    },\n",
    "    {\n",
    "        \"run_name\": \"S2_llrd\",\n",
    "        \"lr\": 2e-5, \"head_lr\": 5e-5, \"layer_decay\": 0.75,\n",
    "        \"weight_decay\": 0.02, \"batch_size\": 32, \"epochs\": 160,\n",
    "        \"augment\": True, \"weighted_sampler\": True, \"freeze_backbone\": False,\n",
    "        \"label_smoothing\": 0.05, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.10\n",
    "    },\n",
    "    {\"run_name\": \"A_baseline_lr1e-5_b32_e60_wd0.01_augY_freezeN\",\n",
    "     \"lr\": 1e-5, \"weight_decay\": 0.01, \"batch_size\": 32, \"epochs\": 60,\n",
    "     \"augment\": True, \"weighted_sampler\": False, \"freeze_backbone\": False,\n",
    "     \"label_smoothing\": 0.0, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.0, \"seed\": 42},\n",
    "\n",
    "    {\"run_name\": \"S_lr3e-5_b64_e60_wd0.01_augY_freezeN_ls0.05\",\n",
    "     \"lr\": 3e-5, \"weight_decay\": 0.01, \"batch_size\": 64, \"epochs\": 60,\n",
    "     \"augment\": True, \"weighted_sampler\": False, \"freeze_backbone\": False,\n",
    "     \"label_smoothing\": 0.05, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.05, \"seed\": 42},\n",
    "\n",
    "    {\"run_name\": \"S_lr2e-5_b32_e80_wd0.01_augY_freezeN_sampler\",\n",
    "     \"lr\": 2e-5, \"weight_decay\": 0.01, \"batch_size\": 32, \"epochs\": 80,\n",
    "     \"augment\": True, \"weighted_sampler\": True, \"freeze_backbone\": False,\n",
    "     \"label_smoothing\": 0.05, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.1, \"seed\": 123},\n",
    "\n",
    "    {\"run_name\": \"S_freeze_lr5e-5_b64_e40_wd0.02_augY_freezeY\",\n",
    "     \"lr\": 5e-5, \"weight_decay\": 0.02, \"batch_size\": 64, \"epochs\": 40,\n",
    "     \"augment\": True, \"weighted_sampler\": False, \"freeze_backbone\": True,\n",
    "     \"label_smoothing\": 0.1, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.1, \"seed\": 7},\n",
    "\n",
    "    {\"run_name\": \"S_noaug_lr1e-5_b32_e60_wd0.01_freezeN_accum4\",\n",
    "     \"lr\": 1e-5, \"weight_decay\": 0.01, \"batch_size\": 32, \"epochs\": 60,\n",
    "     \"augment\": False, \"weighted_sampler\": False, \"freeze_backbone\": False,\n",
    "     \"label_smoothing\": 0.0, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.0, \"grad_accum\": 4, \"seed\": 42},\n",
    "]\n",
    "\n",
    "ALL_SUMMARIES = []\n",
    "for i, cfg in enumerate(SWEEP, start=1):\n",
    "    s, rdir = run_experiment(cfg, i)\n",
    "    s[\"run_dir\"] = str(rdir)\n",
    "    ALL_SUMMARIES.append(s)\n",
    "\n",
    "table_path = Path(RESULTS_ROOT) / \"all_runs_summary.csv\"\n",
    "with open(table_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = [\"run\", \"run_dir\",\n",
    "              \"train_acc\",\"train_auc\",\"train_f1w\",\"train_f1m\",\"train_precm\",\"train_recm\",\"train_mcc\",\"train_kappa\",\n",
    "              \"val_acc\",\"val_auc\",\"val_f1w\",\"val_f1m\",\"val_precm\",\"val_recm\",\"val_mcc\",\"val_kappa\",\n",
    "              \"test_acc\",\"test_auc\",\"test_f1w\",\"test_f1m\",\"test_precm\",\"test_recm\",\"test_mcc\",\"test_kappa\"]\n",
    "    writer.writerow(header)\n",
    "    for idx, s in enumerate(ALL_SUMMARIES, start=1):\n",
    "        row = [idx, s[\"run_dir\"]]\n",
    "        for split in [\"train\",\"val\",\"test\"]:\n",
    "            m = s[split]\n",
    "            row.extend([m[\"accuracy\"], m[\"auc\"], m[\"f1_weighted\"], m[\"f1_macro\"],\n",
    "                        m[\"precision_macro\"], m[\"recall_macro\"], m[\"matthews_corrcoef\"], m[\"cohen_kappa\"]])\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Done. Per-run folders saved under:\", RESULTS_ROOT)\n",
    "print(\"Combined table:\", table_path)\n",
    "\n",
    "def fmt(v):\n",
    "    return f\"{v:.4f}\" if isinstance(v, (float, int, np.floating)) else str(v)\n",
    "\n",
    "print(\"\\n===== SUMMARY (key metrics) =====\")\n",
    "for i, s in enumerate(ALL_SUMMARIES, start=1):\n",
    "    tr, va, te = s[\"train\"], s[\"val\"], s[\"test\"]\n",
    "    print(f\"Run {i}: dir={s['run_dir']}\")\n",
    "    print(f\"  Train: acc {fmt(tr['accuracy'])}, f1w {fmt(tr['f1_weighted'])}, auc {fmt(tr['auc'])}\")\n",
    "    print(f\"  Val  : acc {fmt(va['accuracy'])}, f1w {fmt(va['f1_weighted'])}, auc {fmt(va['auc'])}\")\n",
    "    print(f\"  Test : acc {fmt(te['accuracy'])}, f1w {fmt(te['f1_weighted'])}, auc {fmt(te['auc'])}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
