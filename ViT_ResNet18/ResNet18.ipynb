{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Njukk5pvjBZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, cohen_kappa_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "DATA_DIR = \"/content/gdrive/MyDrive/1segment_topomap_5channel_11classes\"\n",
        "RESULTS_ROOT = \"./resnet_topomap_results_sweep\"\n",
        "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 60\n",
        "NUM_CLASSES = 3\n",
        "IMAGE_SIZE = 224\n",
        "RANDOM_SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "THREE_BAND_IDX = [1, 2, 3]\n",
        "EXCLUDED_SUBJECTS = {\"S038\", \"S088\", \"S089\", \"S092\", \"S100\", \"S104\"}\n",
        "LABEL_MAP = {2: 0, 3: 1, 6: 2, 7: 1}\n",
        "VALID_CLASSES = set(LABEL_MAP.keys())\n",
        "\n",
        "def get_subject_ids():\n",
        "    ids = set()\n",
        "    for fname in os.listdir(DATA_DIR):\n",
        "        if fname.endswith(\".npz\") and fname.startswith(\"sample\"):\n",
        "            sid = fname.split(\"_\")[1][:4]\n",
        "            if sid not in EXCLUDED_SUBJECTS:\n",
        "                ids.add(sid)\n",
        "    return sorted(ids)\n",
        "\n",
        "subject_ids = get_subject_ids()\n",
        "random.seed(RANDOM_SEED)\n",
        "random.shuffle(subject_ids)\n",
        "\n",
        "def _slice(lst, a, b):\n",
        "    a = min(a, len(lst)); b = min(b, len(lst))\n",
        "    return lst[a:b]\n",
        "\n",
        "train_ids = _slice(subject_ids, 0, 70)\n",
        "dev_ids   = _slice(subject_ids, 71, 86)\n",
        "test_ids  = _slice(subject_ids, 87, 103)\n",
        "\n",
        "def load_chunks_by_subject_ids(subject_ids):\n",
        "    X_all, y_all = [], []\n",
        "    files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".npz\") and f.startswith(\"sample\")]\n",
        "    for sid in subject_ids:\n",
        "        sid_token = f\"_{sid}R\"\n",
        "        sid_files = [f for f in files if sid_token in f]\n",
        "        for fname in sid_files:\n",
        "            path = os.path.join(DATA_DIR, fname)\n",
        "            data = np.load(path)\n",
        "            X, y = data[\"X\"], data[\"y\"]\n",
        "            if isinstance(y, np.ndarray):\n",
        "                y = y.item()\n",
        "            if y not in VALID_CLASSES:\n",
        "                continue\n",
        "            if X.ndim != 3 or X.shape[0] <= max(THREE_BAND_IDX):\n",
        "                continue\n",
        "            X = X[THREE_BAND_IDX, :, :]\n",
        "            X = (X - X.mean()) / (X.std() + 1e-8)\n",
        "            X_all.append(X.astype(np.float32))\n",
        "            y_all.append(LABEL_MAP[y])\n",
        "    if not X_all:\n",
        "        raise RuntimeError(\"No samples after subject-based loading.\")\n",
        "    X_all = np.stack(X_all)\n",
        "    y_all = np.array(y_all, dtype=int)\n",
        "    return X_all, y_all\n",
        "\n",
        "X_train, y_train = load_chunks_by_subject_ids(train_ids)\n",
        "X_val,   y_val   = load_chunks_by_subject_ids(dev_ids)\n",
        "X_test,  y_test  = load_chunks_by_subject_ids(test_ids)\n",
        "\n",
        "def resize_224(x: torch.Tensor) -> torch.Tensor:\n",
        "    x = x.unsqueeze(0)\n",
        "    x = F.interpolate(x, size=(IMAGE_SIZE, IMAGE_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "    return x.squeeze(0)\n",
        "\n",
        "class TensorAug:\n",
        "    def __init__(self, do_aug=True, max_rotate=15.0, hflip_p=0.5):\n",
        "        self.do_aug = do_aug\n",
        "        self.max_rotate = max_rotate\n",
        "        self.hflip_p = hflip_p\n",
        "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = resize_224(x)\n",
        "        if not self.do_aug:\n",
        "            return x\n",
        "        if torch.rand(1).item() < self.hflip_p:\n",
        "            x = torch.flip(x, dims=[2])\n",
        "        angle = (torch.rand(1).item() * 2 * self.max_rotate) - self.max_rotate\n",
        "        x = TF.rotate(x, angle, interpolation=InterpolationMode.BILINEAR, expand=False)\n",
        "        return x\n",
        "\n",
        "train_transform = TensorAug(do_aug=True)\n",
        "eval_transform  = TensorAug(do_aug=False)\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx])\n",
        "        y = torch.tensor(self.y[idx]).long()\n",
        "        if self.transform is not None:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "def compute_full_metrics(y_true, logits):\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
        "    try:\n",
        "        if probs.shape[1] == 2:\n",
        "            auc = roc_auc_score(y_true, probs[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"weighted\")\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, preds),\n",
        "        \"auc\": float(auc),\n",
        "        \"precision_weighted\": precision_score(y_true, preds, average=\"weighted\", zero_division=0),\n",
        "        \"recall_weighted\": recall_score(y_true, preds, average=\"weighted\", zero_division=0),\n",
        "        \"f1_weighted\": f1_score(y_true, preds, average=\"weighted\", zero_division=0),\n",
        "        \"precision_macro\": precision_score(y_true, preds, average=\"macro\", zero_division=0),\n",
        "        \"recall_macro\": recall_score(y_true, preds, average=\"macro\", zero_division=0),\n",
        "        \"f1_macro\": f1_score(y_true, preds, average=\"macro\", zero_division=0),\n",
        "        \"matthews_corrcoef\": matthews_corrcoef(y_true, preds),\n",
        "        \"cohen_kappa\": cohen_kappa_score(y_true, preds),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, preds).tolist(),\n",
        "        \"classification_report\": classification_report(y_true, preds, zero_division=0)\n",
        "    }\n",
        "\n",
        "def build_sampler(y_array):\n",
        "    cw = compute_class_weight(\"balanced\", classes=np.unique(y_array), y=y_array)\n",
        "    sample_weights = torch.tensor(cw, dtype=torch.float32)[torch.tensor(y_array, dtype=torch.long)]\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    return sampler\n",
        "\n",
        "def load_resnet(num_classes, freeze_backbone=False):\n",
        "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    if freeze_backbone:\n",
        "        for name, p in model.named_parameters():\n",
        "            if not name.startswith(\"fc\"):\n",
        "                p.requires_grad = False\n",
        "    return model\n",
        "\n",
        "def run_experiment(cfg):\n",
        "    seed = cfg.get(\"seed\", RANDOM_SEED)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    run_name = cfg.get(\"run_name\", f\"run_{int(time.time())}\")\n",
        "    out_dir = os.path.join(RESULTS_ROOT, run_name)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    tr_transform = train_transform if cfg.get(\"augment\", True) else TensorAug(do_aug=False)\n",
        "    train_ds = EEGDataset(X_train, y_train, transform=tr_transform)\n",
        "    val_ds   = EEGDataset(X_val,   y_val,   transform=eval_transform)\n",
        "    test_ds  = EEGDataset(X_test,  y_test,  transform=eval_transform)\n",
        "\n",
        "    if cfg.get(\"weighted_sampler\", False):\n",
        "        sampler = build_sampler(y_train)\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.get(\"batch_size\", BATCH_SIZE), sampler=sampler, num_workers=2, pin_memory=True)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.get(\"batch_size\", BATCH_SIZE), shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader  = DataLoader(val_ds,   batch_size=cfg.get(\"batch_size\", BATCH_SIZE), shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds,  batch_size=cfg.get(\"batch_size\", BATCH_SIZE), shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = load_resnet(NUM_CLASSES, freeze_backbone=cfg.get(\"freeze_backbone\", False)).to(DEVICE)\n",
        "\n",
        "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=float(cfg.get(\"label_smoothing\", 0.0)))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.get(\"lr\", 1e-4), weight_decay=cfg.get(\"weight_decay\", 0.0))\n",
        "    if cfg.get(\"lr_schedule\", \"cosine\") == \"cosine\":\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.get(\"epochs\", EPOCHS))\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "    logs = []\n",
        "\n",
        "    grad_accum = int(cfg.get(\"grad_accum\", 1))\n",
        "    epochs = int(cfg.get(\"epochs\", EPOCHS))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        steps = 0\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for i, (xb, yb) in enumerate(train_loader):\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            loss = loss_fn(out, yb) / grad_accum\n",
        "            loss.backward()\n",
        "            if (i + 1) % grad_accum == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "            running_loss += loss.item()\n",
        "            steps += 1\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits = []\n",
        "            val_labels = []\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(DEVICE)\n",
        "                out = model(xb)\n",
        "                val_logits.append(out.cpu().numpy())\n",
        "                val_labels.append(yb.numpy())\n",
        "            val_logits = np.concatenate(val_logits, axis=0)\n",
        "            val_labels = np.concatenate(val_labels, axis=0)\n",
        "            val_metrics = compute_full_metrics(val_labels, val_logits)\n",
        "        logs.append({\"epoch\": epoch + 1, \"train_loss\": float(running_loss / max(1, steps)), \"val_f1_weighted\": float(val_metrics[\"f1_weighted\"]), \"val_accuracy\": float(val_metrics[\"accuracy\"])})\n",
        "        if val_metrics[\"f1_weighted\"] > best_f1:\n",
        "            best_f1 = val_metrics[\"f1_weighted\"]\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    def eval_split(loader):\n",
        "        model.eval()\n",
        "        logits = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in loader:\n",
        "                xb = xb.to(DEVICE)\n",
        "                out = model(xb)\n",
        "                logits.append(out.cpu().numpy())\n",
        "                labels.append(yb.numpy())\n",
        "        logits = np.concatenate(logits, axis=0)\n",
        "        labels = np.concatenate(labels, axis=0)\n",
        "        return compute_full_metrics(labels, logits)\n",
        "\n",
        "    train_metrics = eval_split(train_loader)\n",
        "    val_metrics   = eval_split(val_loader)\n",
        "    test_metrics  = eval_split(test_loader)\n",
        "\n",
        "    with open(os.path.join(out_dir, \"train_state.json\"), \"w\") as f:\n",
        "        json.dump(logs, f, indent=2)\n",
        "    with open(os.path.join(out_dir, \"results.json\"), \"w\") as f:\n",
        "        json.dump({\"config\": cfg, \"train_metrics\": train_metrics, \"val_metrics\": val_metrics, \"test_metrics\": test_metrics}, f, indent=2)\n",
        "    torch.save(model.state_dict(), os.path.join(out_dir, \"best_model.pth\"))\n",
        "\n",
        "    summary_keys = [\"accuracy\",\"auc\",\"f1_weighted\",\"f1_macro\",\"precision_macro\",\"recall_macro\",\"matthews_corrcoef\",\"cohen_kappa\"]\n",
        "    return {\n",
        "        \"config\": cfg,\n",
        "        \"train\": {k: float(train_metrics[k]) for k in summary_keys},\n",
        "        \"val\":   {k: float(val_metrics[k]) for k in summary_keys},\n",
        "        \"test\":  {k: float(test_metrics[k]) for k in summary_keys},\n",
        "        \"run_dir\": out_dir\n",
        "    }\n",
        "\n",
        "SWEEP = [\n",
        "    {\"run_name\": \"A_baseline_lr1e-4_b32_e60_wd0.01_augY_freezeN\",\n",
        "     \"lr\": 1e-4, \"weight_decay\": 0.01, \"batch_size\": 32, \"epochs\": 60,\n",
        "     \"augment\": True, \"weighted_sampler\": False, \"freeze_backbone\": False,\n",
        "     \"label_smoothing\": 0.0, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.0, \"seed\": 42},\n",
        "\n",
        "    {\"run_name\": \"S_lr3e-4_b64_e60_wd0.01_augY_freezeN_ls0.05\",\n",
        "     \"lr\": 3e-4, \"weight_decay\": 0.01, \"batch_size\": 64, \"epochs\": 60,\n",
        "     \"augment\": True, \"weighted_sampler\": False, \"freeze_backbone\": False,\n",
        "     \"label_smoothing\": 0.05, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.05, \"seed\": 42},\n",
        "\n",
        "    {\"run_name\": \"S_lr2e-4_b32_e80_wd0.01_augY_freezeN_sampler\",\n",
        "     \"lr\": 2e-4, \"weight_decay\": 0.01, \"batch_size\": 32, \"epochs\": 80,\n",
        "     \"augment\": True, \"weighted_sampler\": True, \"freeze_backbone\": False,\n",
        "     \"label_smoothing\": 0.05, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.1, \"seed\": 123},\n",
        "\n",
        "    {\"run_name\": \"S_freeze_lr5e-4_b64_e40_wd0.02_augY_freezeY\",\n",
        "     \"lr\": 5e-4, \"weight_decay\": 0.02, \"batch_size\": 64, \"epochs\": 40,\n",
        "     \"augment\": True, \"weighted_sampler\": False, \"freeze_backbone\": True,\n",
        "     \"label_smoothing\": 0.1, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.1, \"seed\": 7},\n",
        "\n",
        "    {\"run_name\": \"S_noaug_lr1e-4_b32_e60_wd0.01_freezeN_accum4\",\n",
        "     \"lr\": 1e-4, \"weight_decay\": 0.01, \"batch_size\": 32, \"epochs\": 60,\n",
        "     \"augment\": False, \"weighted_sampler\": False, \"freeze_backbone\": False,\n",
        "     \"label_smoothing\": 0.0, \"lr_schedule\": \"cosine\", \"warmup_ratio\": 0.0, \"grad_accum\": 4, \"seed\": 42},\n",
        "]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    all_summaries = []\n",
        "    for i, cfg in enumerate(SWEEP, start=1):\n",
        "        s = run_experiment(cfg)\n",
        "        all_summaries.append(s)\n",
        "    import csv\n",
        "    table_path = os.path.join(RESULTS_ROOT, \"all_runs_summary.csv\")\n",
        "    with open(table_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        header = [\"run\", \"run_dir\",\n",
        "                  \"train_acc\",\"train_auc\",\"train_f1w\",\"train_f1m\",\"train_precm\",\"train_recm\",\"train_mcc\",\"train_kappa\",\n",
        "                  \"val_acc\",\"val_auc\",\"val_f1w\",\"val_f1m\",\"val_precm\",\"val_recm\",\"val_mcc\",\"val_kappa\",\n",
        "                  \"test_acc\",\"test_auc\",\"test_f1w\",\"test_f1m\",\"test_precm\",\"test_recm\",\"test_mcc\",\"test_kappa\"]\n",
        "        writer.writerow(header)\n",
        "        for idx, s in enumerate(all_summaries, start=1):\n",
        "            row = [idx, s[\"run_dir\"]]\n",
        "            for split in [\"train\",\"val\",\"test\"]:\n",
        "                m = s[split]\n",
        "                row.extend([m[\"accuracy\"], m[\"auc\"], m[\"f1_weighted\"], m[\"f1_macro\"],\n",
        "                            m[\"precision_macro\"], m[\"recall_macro\"], m[\"matthews_corrcoef\"], m[\"cohen_kappa\"]])\n",
        "            writer.writerow(row)\n",
        "    with open(os.path.join(RESULTS_ROOT, \"sweep_summary.json\"), \"w\") as f:\n",
        "        json.dump(all_summaries, f, indent=2)\n"
      ]
    }
  ]
}